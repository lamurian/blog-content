---
author: lam
title: "Mathematics for Machine Learning"
weight: 4
description: >

  Mathematics is the heart of machine learning algorithms. We communicate our
  problem and solution as formal logic, in a way that the computer can
  understand. The scope of mathematics in machine learning is broader than what
  this lecture may have captured. We shall focus on a few branches in
  mathematics, including linear algebra, calculus, probability and statistics.
  The current post has no intention to provide an in-depth review of mathematical
  concepts. Instead, it will serve as a gentle introduction to excite you in
  learning more about mathematics.

summary: >

  Mathematics is the heart of machine learning algorithms. We communicate our
  problem and solution as formal logic, in a way that the computer can
  understand. The scope of mathematics in machine learning is broader than what
  this lecture may have captured. We shall focus on a few branches in
  mathematics, including linear algebra, calculus, probability and statistics.
  The current post has no intention to provide an in-depth review of mathematical
  concepts. Instead, it will serve as a gentle introduction to excite you in
  learning more about mathematics.

date: 2020-10-09
categories: ["AI", "UI", "DV"]
tags: ["mathematics", "machine learning"]
slug: 04-math-ml
csl: ../harvard.csl
bibliography: ../ref.bib
draft: false
---



<p><a href="https://lamurian.rbind.io/note/aidv-fkui/04-math-ml/slide">Slide</a></p>
<p>Mathematics is the heart of machine learning algorithms. We communicate our
problem and solution as formal logic, in a way that the computer can
understand. The scope of mathematics in machine learning is broader than what
this lecture may have captured. We shall focus on a few branches in
mathematics, including linear algebra, calculus, probability and statistics.
The current post has no intention to provide an in-depth review of mathematical
concepts. Instead, it will serve as a gentle introduction to excite you in
learning more about mathematics.</p>
<div id="linear-algebra" class="section level1">
<h1>Linear Algebra</h1>
<p>As a branch of mathematics, linear algebra sets rules in manipulating
mathematical objects within a defined set. Objects in linear algebra involve a
scalar, vector, matrix and tensor. Like other branches in mathematics, linear
algebra governs its own space, which includes a vector, metric, normed line and
inner product space. By defining rules to operate on an object, linear algebra
specify a relationship between multiple variables. Variables are observable
features of our interest, which we will refer to as a dimension from this point
onward. We may represent linear algebra in the following simple equation:</p>
<p><span class="math display">\[\alpha x + \beta y = c\]</span></p>
<p>Please be advised, we often assume flat Euclidean space when constructing
algebraic rules. However, violation on any of Euclid’s postulates may happen in
a non-Euclidean geometry. In such cases, Euclidean approximation does not
generalize well to non-Euclidean space. As a quick reminder, Euclid proposed
five postulates in defining a straight line:</p>
<ol style="list-style-type: decimal">
<li>A straight line segment can be drawn joining any two points.</li>
<li>Any straight line segment can be extended indefinitely in a straight line.</li>
<li>Given any straight lines segment, a circle can be drawn having the segment
as radius and one endpoint as center.</li>
<li>All Right Angles are congruent.</li>
<li>If two lines are drawn which intersect a third in such a way that the sum of
the inner angles on one side is less than two Right Angles, then the two
lines inevitably must intersect each other on that side if extended far
enough. This postulate is equivalent to what is known as the Parallel
Postulate.</li>
</ol>
<p>The first four postulate seems ready to digest and does not violate our
preconception on straight lines. However, the fifth one <em>is</em> a bit horrendous
that we need to carefully read them as to not misinterpret the idea. We can
paraphrase the fifth postulate, in which it defines a behaviour seen in
two parallel lines.</p>
<p><img src="https://i.ytimg.com/vi/0v2FvpTkW7I/maxresdefault.jpg" width="50%"></p>
<p>Now, all of them sounds reasonable, but do they? Unfortunately, Euclid’s
postulates only prevail on a flat surface. With added curvature, i.e. in a
non-Euclidean geometry, we shall observe violations on at least one of them.</p>
<p><img src="https://images.squarespace-cdn.com/content/56ee72d9c2ea51bd675641da/1476242359445-8N16VQ6VAQT38XXNLD8Q/?content-type=image%2Fjpeg" width="100%"></p>
<p>Still, Euclidean space is useful as a model of our real world, where we can
make an approximation of distances between two data points. Most of
well-established machine learning algorithms use Euclidean space in computing
their metrics of distance. Hierarchical clustering and principal component
analysis are to name but a few, as there are still others to jump on the
bandwagon!</p>
<p>Due to the constraint in Euclidean space, data with higher dimension will
suffer from phenomenons unseen in lower dimension, as we call them the curse of
dimensionality. Generally speaking, more dimensions requires a higher volume of
data. Some address this limitation by developing a non-Euclidean algorithm,
e.g. the geometric deep learning. To keep the post brief, we will only consider
an Euclidean space when creating our model. We have seen how algebra relates
with geometry, and we know what their limitation is. Now, we can safely proceed
to comprehend the role of algebra in machine learning :)</p>
<div id="vector-space" class="section level2">
<h2>Vector Space</h2>
<p>In a vector space, algebraic rules govern multiplication and additions,
resulting in objects of the same class. During high school, we may have seen a
few examples on geometric vector, for instances those we recognized from
physics. We know a vector has its magnitude and direction, where opposing
vectors of the same magnitude abolish each other. In mathematics, the classes
are more general, where we can have polynomial, signals and tuples. In essence,
the vector space considers sequences as an object to manipulate. To bring it
into context, machine learning will have tuples as their object in the vector
space. In a system of linear algebra, we define:</p>
<p><span class="math display">\[\begin{align}
Let\ X &amp;= (\mathbf{x_1}, \mathbf{x_2}, ..., \mathbf{x_n}) \\
A &amp;= (\alpha_1, \alpha_2, ..., \alpha_m) : \forall A, X \in \mathbb{R},\ then \\
\exists f(x) &amp;= \displaystyle \sum_1^j \sum_1^i \alpha_{ij} \cdot \mathbf{x}_i = \Pi
\end{align}\]</span></p>
<p>It looks horrible! But rest easy, we can read it this way:</p>
<blockquote>
<p>Let there be a sequence of vector <strong>X</strong> and scalar <span class="math inline">\(A\)</span>, there exists a
function defining the sum of product between <strong>x</strong> and <span class="math inline">\(\alpha\)</span></p>
</blockquote>
<p>Now, they do not seem as bad as they were :) We can simplify the concept
using a quick example:</p>
<p><span class="math display">\[\begin{align}
4x_1 + 4x_2 &amp;= 5 \\
2x_1 - 4x_2 &amp;= 1 \tag{+} \\
\hline
6x_1 &amp;= 6 \\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\therefore x_1 &amp; = 1 \\
x_2 &amp; = 0.25
\end{align}\]</span></p>
<p>I mentioned <em>quick</em>, but I did not mean to be this short! Well, that is
partially because the fundamentals of linear algebra is not a complicated
concept. It is intuitive to solve, even at a glance, we can see an emerging
pattern from our set of equations. However, a computer does not rely on
intuition. As a solution, we teach a computer to solve algebraic problem using
matrices. If we were to re-formulate our problem and solution as a matrix, we
can present them in the following fashion:</p>
<p><span class="math display">\[\begin{align}
\begin{bmatrix} 4 &amp; 4 \\ 2 &amp; -4 \end{bmatrix}
\cdot \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} &amp;=
\begin{bmatrix} 5 \\ 1 \end{bmatrix}
\\
Let\ A &amp;:= \begin{bmatrix} 4 &amp; 4 \\ 2 &amp; -4 \end{bmatrix}
\\
A \cdot \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} &amp;=
\begin{bmatrix} 5 \\ 1 \end{bmatrix}
\\
A^{-1} \cdot A \cdot \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} &amp;=
A^{-1} \begin{bmatrix} 5 \\ 1 \end{bmatrix} \\
\end{align}\]</span></p>
<p>What we need to accomplish now is finding the appropriate inverse matrix
<span class="math inline">\(A^{-1}\)</span>. Recall that <span class="math inline">\(A \cdot A^{-1} = I\)</span>, with <span class="math inline">\(I\)</span> being and identity matrix.
We will start solving the <span class="math inline">\(A^{-1}\)</span> by having the original and identity matrix
side by side. At the end of our procedures, we expect to turn the original
matrix into an identity matrix. Since we will do a row-wise operations, any
changes will occur on both sides. Meaning that, by turning the original into
identity matrix, we will directly change the identity matrix <span class="math inline">\(I\)</span> (right side)
into an inverse matrix <span class="math inline">\(A^{-1}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\left[\begin{array}{cc|cc}
4 &amp; 4 &amp; 1 &amp; 0 \\
2 &amp; -4 &amp; 0 &amp; 1
\end{array}\right]
\end{align}\]</span></p>
<p>Add the second row into the first
<span class="math display">\[\begin{align}
\left[\begin{array}{cc|cc}
6 &amp; 0 &amp; 1 &amp; 1 \\
2 &amp; -4 &amp; 0 &amp; 1
\end{array}\right]
\end{align}\]</span></p>
<p>Divide the first row by 6
<span class="math display">\[\begin{align}
\left[\begin{array}{cc|cc}
1 &amp; 0 &amp; \frac{1}{6} &amp; \frac{1}{6} \\
2 &amp; -4 &amp; 0 &amp; 1
\end{array}\right]
\end{align}\]</span></p>
<p>Divide the second row by 2
<span class="math display">\[\begin{align}
\left[\begin{array}{cc|cc}
1 &amp; 0 &amp; \frac{1}{6} &amp; \frac{1}{6} \\
1 &amp; -2 &amp; 0 &amp; \frac{1}{2}
\end{array}\right]
\end{align}\]</span></p>
<p>Subtract the first row from the second
<span class="math display">\[\begin{align}
\left[\begin{array}{cc|cc}
1 &amp; 0 &amp; \frac{1}{6} &amp; \frac{1}{6} \\
0 &amp; -2 &amp; - \frac{1}{6} &amp; \frac{1}{3}
\end{array}\right]
\end{align}\]</span></p>
<p>Divide the second row by -2
<span class="math display">\[\begin{align}
\left[\begin{array}{cc|cc}
1 &amp; 0 &amp; \frac{1}{6} &amp; \frac{1}{6} \\
0 &amp; 1 &amp; \frac{1}{12} &amp; - \frac{1}{6}
\end{array}\right]
\end{align}\]</span></p>
<p>Now you have <span class="math inline">\(I\)</span> on the left and <span class="math inline">\(A^{-1}\)</span> on the right :) We call this method
an <strong>elementary row operation</strong>. With this method, you can multiply a
particular row using any scalar. However, you <em>may not</em> perform addition nor
subtraction using <em>a constant</em>. Subtraction and addition are permissible if you
do such an operation between rows, e.g. subtracting the first row from the
second one. For the record, swapping the row is perfectly fine though. The
computation may seem daunting, but it is one of the simplest and most explicit
methods to solve inverses in a <span class="math inline">\(m \times n\)</span> matrices. Another method exists,
and I will leave that to your volition to make a venture. Since we have our
inverse matrix <span class="math inline">\(A^{-1}\)</span>, we can solve our problem:</p>
<p><span class="math display">\[\begin{align}
A^{-1} \cdot A \cdot \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} &amp;=
A^{-1} \begin{bmatrix} 5 \\ 1 \end{bmatrix}
\\
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} &amp;=
\begin{bmatrix}\frac{1}{6} &amp; \frac{1}{6} \\ \frac{1}{12} &amp; - \frac{1}{6}\end{bmatrix}
\cdot \begin{bmatrix} 5 \\ 1 \end{bmatrix}
\\
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} &amp;=
\begin{bmatrix} 1 \\ \frac{1}{4} \end{bmatrix}
\end{align}\]</span></p>
<p>It took longer than expected, why do we need to use matrices? Well, just to be
fair, computers understand matrices as a mathematical construct. They can
easily perform mathematical operation, faster than what most of us can keep up
with. Also, matrix is generalizable into a higher dimensional object, i.e. a
tensor. A dot product in matrices is akin to inner product (or tensor product,
if you will) in tensors. But I have to disappoint you with the fact that not
all matrices are invertible :( Only invertible matrices provide a unique
solution for our system of linear equation.</p>
</div>
<div id="metric-space" class="section level2">
<h2>Metric Space</h2>
<p>Still related to the vector space, a metric space is a place where distances
between two data points live in. The metric space has a pair of set and
distance object, where they exhibit particular properties. We denote the metric
space as:</p>
<p><span class="math display">\[\begin{align}
d: X \times X \to [0, \infty)
\end{align}\]</span></p>
<p>With <span class="math inline">\(X\)</span> being a set of real number. Some properties of a distance <span class="math inline">\(d(x, y)\)</span> we
can observe in the metric space are:</p>
<p><span class="math display">\[\begin{align}
Let\ (x, y) &amp; \in X \\
d(x, y) &amp;= 0 \iff x = y \\
d(x, y) &amp; \geqslant 0 \iff x \neq y \\
d(x, y) &amp;= d(y, x) \\
d(x, y) &amp; \leqslant d(x, z) + d(z, y)
\end{align}\]</span></p>
<p>In determining the distance <span class="math inline">\(d(x,y)\)</span>, we can use different approaches based on
what we need to accomplish.</p>
<div id="euclidean-distance" class="section level3">
<h3>Euclidean Distance</h3>
<p><span class="math display">\[D = \sqrt{\displaystyle \sum_{i=1}^n (p_i - q_i)^2}\]</span></p>
<ul>
<li>Measure the shortest distance in an .amber[Euclidean] space</li>
<li>Most commonly used distance measure</li>
<li><span class="math inline">\(n\)</span> is the number of dimension</li>
</ul>
</div>
<div id="manhattan-distance" class="section level3">
<h3>Manhattan Distance</h3>
<p><span class="math display">\[D = \displaystyle \sum_{i=1}{n} |p_i - q_i|\]</span></p>
<ul>
<li>An absolute distance in an .amber[Euclidean] space</li>
<li>Optimally used in a higher dimension data</li>
</ul>
</div>
<div id="minkowski-distance" class="section level3">
<h3>Minkowski Distance</h3>
<p><span class="math display">\[D = \bigg( \displaystyle \sum_{i=1}^n |p_i - q_i|^p \bigg)^{\frac{1}{p}}\]</span></p>
<ul>
<li>Useful in a .amber[non-euclidean] space</li>
<li>Not a part of metric space, but still nice to know</li>
<li><span class="math inline">\(p\)</span>: Order of the norm</li>
<li>A more general form of the former two distance measures</li>
<li>When <span class="math inline">\(p=1\)</span>, it behaves as a Manhattan distance</li>
<li>When <span class="math inline">\(p=2\)</span>, it behaves as an Euclidean distance</li>
</ul>
</div>
<div id="comparison-on-distance-measures" class="section level3">
<h3>Comparison on Distance Measures</h3>
<p><img src="https://www.researchgate.net/publication/329452521/figure/fig1/AS:700891866877954@1544117051744/A-comparison-of-the-actual-road-Euclidean-Minkowski-and-Manhattan-distances-between-two.png" width="60%"></p>
</div>
</div>
<div id="normed-linear-space" class="section level2">
<h2>Normed Linear Space</h2>
<p>A normed linear space is a non-negative map on the <strong>vector space</strong>, where it
formalizes the length of <span class="math inline">\(x\)</span>. Using a norm, a real-valued function, it
approximates a real-world length of <span class="math inline">\(x\)</span> as measured from the starting point
<span class="math inline">\(0\)</span>. An example of normed linear space includes the Hilbert space and <span class="math inline">\(L^p\)</span>
space.</p>
</div>
<div id="inner-product-space" class="section level2">
<h2>Inner Product Space</h2>
<p>In essence, the inner product space introduces inner products into a vector
space (duh!). It maps two vectors into a scalar, where we define the inner
product as a generalization of a dot product. Please kindly recall our
discussion on tensor products. So, we can imagine an inner product as a dot
product without dimensionality constraint</p>
</div>
</div>
<div id="calculus" class="section level1">
<h1>Calculus</h1>
<p>The role of calculus in machine learning is to optimize the modelled algorithm.
Calculus can minimize the cost function by adjusting weights used in the model.
It aims to find extremum, either as a minima or maxima. Finding the extremum
highly depends on which cost function assigned to assess the model. In general,
exists are two type of cost function, i.e. a convex and concave function. We
are looking for a minima in a <em>convex</em> function and a maxima in a <em>concave</em>
function. In doing so, we are performing a gradient function, i.e. a set of
partial derivations to find the optimal values.</p>
<p><img src="https://miro.medium.com/max/1200/1*9Fca3kpx3pVW8SaYz2pjpw.png" width="90%"></p>
<p><span class="math display">\[\begin{align}
\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
\end{align}\]</span></p>
</div>
<div id="probability-and-statistics" class="section level1">
<h1>Probability and Statistics</h1>
<p><span class="math display">\[P(A|B) = P(A) \cdot P(B)\]</span></p>
<p>Both frequentist and Bayesian statistics revolve around conditional
probabilities. The only difference is, Bayesian statistics allows us to update
our prior belief on the probability distribution. But, it is pivotal to
understand the question: what is statistic in essence? Statistics is an
approximation of parameters in population, which we extensively discussed in
the <a href="https://lamurian.rbind.io/note/biostat-ukrida/">biostatistic course</a>.</p>
</div>
