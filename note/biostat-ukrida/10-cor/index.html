---
author: lam
title: "Correlation of Numeric Variables"
weight: 10
description: >

  So far, we have solved hypotheses testings for condition where we have numeric
  values as our dependent variable and categoric data as our independent
  variables. We have yet to see the solution if we have numeric variables for
  both of the dependent and independent variables. After learning the descriptive
  statistics, we understand that we can observe the spread in our data by
  measuring the variance, i.e. the dispersion of each data element relative to
  the mean value. Similarly, we can understand the dispersion of two numeric
  variables by accounting the covariance.

summary: >

  So far, we have solved hypotheses testings for condition where we have numeric
  values as our dependent variable and categoric data as our independent
  variables. We have yet to see the solution if we have numeric variables for
  both of the dependent and independent variables. After learning the descriptive
  statistics, we understand that we can observe the spread in our data by
  measuring the variance, i.e. the dispersion of each data element relative to
  the mean value. Similarly, we can understand the dispersion of two numeric
  variables by accounting the covariance.

date: 2020-12-03
categories: ["statistics", "ukrida"]
tags: ["R", "hypothesis"]
slug: 10-cor
csl: ../harvard.csl
bibliography: ../ref.bib
draft: false
---



<p><a href="https://lamurian.rbind.io/note/biostat-ukrida/10-cor/slide">Slide</a></p>
<p>So far, we have solved hypotheses testings for condition where we have numeric
values as our dependent variable and categoric data as our independent
variables. We have yet to see the solution if we have numeric variables for
both of the dependent and independent variables. After learning the descriptive
statistics, we understand that we can observe the spread in our data by
measuring the variance, i.e. the dispersion of each data element relative to
the mean value. Similarly, we can understand the dispersion of two numeric
variables by accounting the covariance.</p>
<div id="covariance" class="section level1">
<h1>Covariance</h1>
<p>The covariance describes the length of relationship between two numeric
variables. In calculating and interpreting covariance, we are interested to
measure the sign, a trend where one variable may vary depends on the other. The
question we wish to address is how a variable <span class="math inline">\(y\)</span> will behave if we know the
value of <span class="math inline">\(x\)</span>. In covariance, the magnitude is not directly interpretable, as it
is not scale free. It is more intuitive to understand covariance when we know
its equation, presented as follow:</p>
<p><span class="math display">\[\sigma_{x, y} = \frac{\displaystyle \sum_{i=1}^n(x_i - \mu_x)(y_i - \mu_y)}{n}\]</span></p>
<p>We see from the enumerator that the covariance takes into account the residual
of each data element relative to the mean value. We can imagine that due to the
difference of scale between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, the output of covariance calculation
will not clearly reflect the magnitude of relationship. Suppose we have two
dataset, each with <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> following a different scale. If the first
dataset have <span class="math inline">\(x \in [100, 200]\)</span> and <span class="math inline">\(y \in [1000, 2000]\)</span> while the second one
have <span class="math inline">\(x \in [50, 100]\)</span> and <span class="math inline">\(y \in [20, 50]\)</span>, we will have a different
covariance value even though both has roughly similar linear relationship.
Linearity is the important bit in covariance, where we assume both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>
follow a similar trend. Please be advised though, as in calculating the
variance, the sampled covariance will require a Bessel’s correction, where we
will use the following equation to measure covariance in a sample:</p>
<p><span class="math display">\[s_{x, y} = \frac{\displaystyle \sum_{i=1}^n(x_i - {\bar{x}}) (y_i - {\bar{y}})}{({n-1})}\]</span></p>
<p>Another important concept we need to understand in calculating covariance is
the covariance matrix. In this matrix, we can take a glimpse of the general
relationship between multiple pairs of numeric variables. Having a covariance
matrix is crucial to determine the overall trend at a glance. Covariance matrix
provides a useful descriptive statistics before designing a complex model,
which we will learn in the next lectures on the linear model (lm) and
generalized linear model (glm).</p>
<div id="example-please" class="section level2">
<h2>Example, please?</h2>
<pre class="r"><code>tbl &lt;- subset(iris, select=c(Sepal.Width, Sepal.Length)) %&gt;% str()</code></pre>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  2 variables:
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...</code></pre>
<p>In this example, we will use a subset of the <code>iris</code> dataset, which we have
grown accustomed to. This subset will only include two variables, <code>Sepal.Width</code>
and <code>Sepal.Length</code>. From this point onwards, we will set <code>x</code> to represent the
width and <code>y</code> to represent the length. To help us visualize the concept, we
will calculate the residual to the mean as <code>x.resid</code> and <code>y.resid</code>, each
reflected <span class="math inline">\(x_i - \bar{x}\)</span> and <span class="math inline">\(y_i - \bar{y}\)</span>. The following table describes
the first six rows in our data:</p>
<table>
<thead>
<tr>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
y
</th>
<th style="text-align:right;">
x.resid
</th>
<th style="text-align:right;">
y.resid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
5.1
</td>
<td style="text-align:right;">
3.5
</td>
<td style="text-align:right;">
-0.74
</td>
<td style="text-align:right;">
0.44
</td>
</tr>
<tr>
<td style="text-align:right;">
4.9
</td>
<td style="text-align:right;">
3.0
</td>
<td style="text-align:right;">
-0.94
</td>
<td style="text-align:right;">
-0.06
</td>
</tr>
<tr>
<td style="text-align:right;">
4.7
</td>
<td style="text-align:right;">
3.2
</td>
<td style="text-align:right;">
-1.14
</td>
<td style="text-align:right;">
0.14
</td>
</tr>
<tr>
<td style="text-align:right;">
4.6
</td>
<td style="text-align:right;">
3.1
</td>
<td style="text-align:right;">
-1.24
</td>
<td style="text-align:right;">
0.04
</td>
</tr>
<tr>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
3.6
</td>
<td style="text-align:right;">
-0.84
</td>
<td style="text-align:right;">
0.54
</td>
</tr>
<tr>
<td style="text-align:right;">
5.4
</td>
<td style="text-align:right;">
3.9
</td>
<td style="text-align:right;">
-0.44
</td>
<td style="text-align:right;">
0.84
</td>
</tr>
</tbody>
</table>
<p>Now, we will make a function describing the covariance equation. As previously
described, this function will take two arguments, <code>x</code> and <code>y</code>. Then it will
calculate the mean and residual to the mean for each <code>x</code> and <code>y</code>. Here, the
value of both inputs are vectors. Then we get the product of each residual from
<code>x</code> and <code>y</code>, where the addition of all products will divide by <span class="math inline">\(n-1\)</span>.</p>
<pre class="r"><code>covariance &lt;- function(x, y) {
    n &lt;- length(x) # Length of x must be = length of y
    {(x - mean(x)) * (y - mean(y))} %&gt;% sum() %&gt;% divide_by(n-1)
}</code></pre>
<p>We can compare the covariance from our function and the covariance
resulted from a built-in function in <code>R</code>.</p>
<pre class="r"><code>covariance(tbl$x, tbl$y)</code></pre>
<pre><code>## [1] -0.042</code></pre>
<pre class="r"><code>cov(tbl$x, tbl$y) # Built-in function</code></pre>
<pre><code>## [1] -0.042</code></pre>
<p>It is also interesting to explore what will happen if we calculate covariances of
the same variable. As it turns out, the covariance of the same variable is its
variance!</p>
<pre class="r"><code>covariance(tbl$x, tbl$x)</code></pre>
<pre><code>## [1] 0.69</code></pre>
<pre class="r"><code>var(tbl$x) # Variance of x</code></pre>
<pre><code>## [1] 0.69</code></pre>
<p>It is pretty straightforward if we closely evaluate the equation:</p>
<p><span class="math display">\[s_{x, x} = \frac{\displaystyle \sum_{i=1}^n(x_i - {\bar{x}}) (x_i - {\bar{x}})}{({n-1})}\]</span></p>
<p>Previously, we mentioned a concept on covariance matrices. Of course we can do
that to in <code>R</code> using the <code>cov</code> function.</p>
<pre class="r"><code>tbl &lt;- subset(iris, select=-Species) %T&gt;% str()</code></pre>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  4 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...</code></pre>
<pre class="r"><code>cov(tbl)</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length        0.686      -0.042         1.27        0.52
## Sepal.Width        -0.042       0.190        -0.33       -0.12
## Petal.Length        1.274      -0.330         3.12        1.30
## Petal.Width         0.516      -0.122         1.30        0.58</code></pre>
</div>
</div>
<div id="pearsons-r" class="section level1">
<h1>Pearson’s <span class="math inline">\(r\)</span></h1>
<p>Correlation is a step after covariance, where we can employ Pearson’s method to
measure the trend and magnitude of a relationship between two numeric
variables. Correlation is dimension free, as it takes into account the standard
deviation on how both variables differ from each other. In a way, correlation
is the degree of relationship between two standardized score, both estimated in
a Z distribution.</p>
<p><span class="math display">\[\begin{align}
r &amp;= \frac{{s_{x,y}}}{s_x \cdot s_y} \\
  &amp;= \displaystyle \sum_{i=1}^n \frac{{(x-\bar{x}) (y-\bar{y})}} {{(n-1)} \cdot s_x \cdot s_y} \\
  &amp;= \displaystyle \sum_{i=1}^n \frac{\big( \frac{x-\bar{x}}{s_x} \big) \cdot \big( \frac{y-\bar{y}}{s_y} \big)}{n-1} \\
  &amp;= \frac{Z_x \cdot Z_y}{n-1} \\
\\
\nu &amp;= n - 2 \tag{DoF}
\end{align}\]</span></p>
<p>As it employs a Z-score to measure the relationship between two values,
normally-distributed variables will have their association fully described
using Pearson’s <span class="math inline">\(r\)</span> method. Please recall that <span class="math inline">\(Z \sim N(0,1)\)</span>, since <span class="math inline">\(r \to Z\)</span>, <span class="math inline">\(r\)</span> does not care for the measurement unit, i.e. scale free.</p>
<p><span class="math display">\[t = \frac{r}{\sqrt{\frac{1-r^2}{n-2}}}\]</span></p>
<p>To denote significance, we shall calculate the <span class="math inline">\(t: t \sim T(\nu)\)</span> statistics.
Please be advised, there exists another method of determining the significance,
but we only consider the simplest method available. In calculating the
correlation, we assume:</p>
<ul>
<li>I.I.D</li>
<li>Univariate normality</li>
<li>Bivariate normality</li>
<li>Has a linear relationship</li>
</ul>
<p>The important concept we need to highlight in this section is the presence of a
joint distribution, i.e. the bivariate normality assumption. Suppose we have
two variables of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, then we understand each variables has its own
distribution. However, when we <em>jointly</em> distribute each data point, i.e. by
assigning a pair of <span class="math inline">\((x_i, y_i)\)</span>, we will obtain a joint distribution. A joint
distribution may follow a certain probability characteristic, and to utilize
the full extent of Pearson’s <span class="math inline">\(r\)</span> correlation, we expect to have a normal joint
distribution of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Another important concept here is linearity, where
we can eyeball the relationship using a scatter plot. In linearity assumption,
we shall visualize a linear pattern depicting a relationship of both variables.
Fulfilling required assumptions, we can proceed with formulating the
hypotheses:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: Both variables do not have a linear relationship</li>
<li><span class="math inline">\(H_1\)</span>: Both variables have a linear relationship</li>
</ul>
<div id="example-please-1" class="section level2">
<h2>Example, please?</h2>
<p>This example will use variables from the previous one, where we first tested
for the univariate normality.</p>
<pre class="r"><code>lapply(tbl, shapiro.test) %&gt;% lapply(broom::tidy) %&gt;% lapply(data.frame) %&gt;%
    {do.call(rbind, .)} %&gt;% kable()</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:left;">
method
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Sepal.Length
</td>
<td style="text-align:right;">
0.98
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
Sepal.Width
</td>
<td style="text-align:right;">
0.98
</td>
<td style="text-align:right;">
0.10
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
Petal.Length
</td>
<td style="text-align:right;">
0.88
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
Petal.Width
</td>
<td style="text-align:right;">
0.90
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
</tbody>
</table>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/pearson2-1.png" width="100%" /></p>
<p>From both Shapiro-Wilk test and a visual examination, we may conclude that the
<code>Sepal.Width</code> follows a normal distribution and <code>Sepal.Length</code> is seemingly
close to the normal distribution. Using a QQ-plot, we do not observe many
normality violations in the <code>Sepal.Length</code> variable. Then we proceeded
with the bivariate normality assumption.</p>
<pre class="r"><code>subset(tbl, select=c(Sepal.Length, Sepal.Width)) %&gt;%
    MVN::mvn() # Multivariate normality</code></pre>
<pre><code>## $multivariateNormality
##            Test  HZ p value MVN
## 1 Henze-Zirkler 2.9   8e-07  NO
## 
## $univariateNormality
##               Test     Variable Statistic   p value Normality
## 1 Anderson-Darling Sepal.Length      0.89     0.022    NO    
## 2 Anderson-Darling Sepal.Width       0.91     0.020    NO    
## 
## $Descriptives
##                n Mean Std.Dev Median Min Max 25th 75th Skew Kurtosis
## Sepal.Length 150  5.8    0.83    5.8 4.3 7.9  5.1  6.4 0.31    -0.61
## Sepal.Width  150  3.1    0.44    3.0 2.0 4.4  2.8  3.3 0.31     0.14</code></pre>
<p>Using the Mardia’s test to calculate skewness and kurtosis of a joint
distribution, we see that a joint distribution of <code>Sepal.Width</code> and
<code>Sepal.Length</code> follows the normal distribution. In <code>R</code>, calculating correlation
is a simple task using the <code>cor.test()</code> function, where it takes two necessary
arguments of sample variables.</p>
<pre class="r"><code>cor.test(tbl$Sepal.Length, tbl$Sepal.Width)</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  tbl$Sepal.Length and tbl$Sepal.Width
## t = -1, df = 148, p-value = 0.2
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.273  0.044
## sample estimates:
##   cor 
## -0.12</code></pre>
<p>The correlation coefficient <span class="math inline">\(r\)</span> will have a range of value <span class="math inline">\([-1, 1]\)</span>, where it
reflects the trend and magnitude of a relationship between two numeric
variables.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/pearson4-1.png" width="100%" /></p>
<p>This plot visualizes the linearity and the extent of correlation between
<code>Sepal.Length</code> and <code>Sepal.Width</code>.</p>
</div>
</div>
<div id="spearmans-rho" class="section level1">
<h1>Spearman’s <span class="math inline">\(\rho\)</span></h1>
<p>As Pearson’s <span class="math inline">\(r\)</span> is a parametric test, we have stringent assumptions to follow.
In case we could not satisfy the assumption, i.e. when we have an ordinal data
or the relationship is not completely linear, it is best to employ a
non-parametric method. Spearman’s <span class="math inline">\(\rho\)</span> is the non-parametric correlation
test, suitable to handle ordinal data. In some cases, Spearman’s <span class="math inline">\(\rho\)</span>
performs considerably well to measure correlation between non-normally
distributed numeric data. However, Spearman’s <span class="math inline">\(\rho\)</span> method is not sufficient
to correctly handle tied values, as we will see in its equation.</p>
<p><span class="math display">\[\begin{align}
\rho &amp;= 1 - \frac{6 \sum (R_x - R_y)^2}{n (n^2 - 1)} \\
\nu  &amp;= n - 2 \tag{DoF}
\end{align}\]</span></p>
<p>In calculating <span class="math inline">\(\rho\)</span>, the value of <span class="math inline">\(R_{x, y}\)</span> is the rank for variables <span class="math inline">\(x\)</span>
and <span class="math inline">\(y\)</span>. Ranking in Spearman’s method follows an order within one variable,
i.e. not the pooled data. By assigning rank, we can address non-linearity to a
certain degree. As an alternative to this equation, we can use Pearson’s
method while substituting the actual value to its rank. This method handles
tied values by taking the average rank (kindly refer to our discussion in
non-parametric mean difference topics).</p>
<p><span class="math display">\[t = \frac{\rho}{\sqrt{\frac{1-\rho^2}{n-2}}}\]</span></p>
<p>As in Pearson’s <span class="math inline">\(r\)</span>, we measure the significance by calculating the <span class="math inline">\(t\)</span>
statistics. In the presence of tie, Spearman’s method faces difficulties in
confidently determine the p-value. Even though not assuming the normality,
there be a few assumption we need to satisfy so that we can use the Spearman’s
method correctly:</p>
<ul>
<li>I.I.D</li>
<li>Monotonic trend</li>
<li>Has a natural order</li>
</ul>
<p>Here we introduced a new terminology, <em>viz.</em> monotonic trend. As to make things
a bit easier to understand, please know that linearity is a subset of a
monotonic trend. Monotonic trend simply describes a consistent trend between
two variables, either be an upward or downward slope. Since monotonic trend is
a general form of a linear trend, it does not have to fully address linearity.
It means that we can expect a slight curvature in a monotonic trend as long as
it presents with a overtime consistent change.</p>
<div id="example-please-2" class="section level2">
<h2>Example, please?</h2>
<p>This example is only for an illustrative purpose, as we agreed that Spearman’s
method finds its best uses to measure correlation in an ordinal data. We keep
on using the subset of an <code>iris</code> dataset just to keep this example simple.</p>
<pre class="r"><code>tbl &lt;- subset(iris, select=-Species) %T&gt;% str()</code></pre>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  4 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...</code></pre>
<pre class="r"><code>cov(tbl)</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length        0.686      -0.042         1.27        0.52
## Sepal.Width        -0.042       0.190        -0.33       -0.12
## Petal.Length        1.274      -0.330         3.12        1.30
## Petal.Width         0.516      -0.122         1.30        0.58</code></pre>
<pre class="r"><code>cor.test(tbl$Sepal.Length, tbl$Sepal.Width, method=&quot;spearman&quot;)</code></pre>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  tbl$Sepal.Length and tbl$Sepal.Width
## S = 7e+05, p-value = 0.04
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##   rho 
## -0.17</code></pre>
<p>As in <span class="math inline">\(r\)</span>, the value of <span class="math inline">\(\rho\)</span> will fall in the range between <span class="math inline">\([-1, 1]\)</span>, where
it respectively describes a negative and positive trend. The magnitude in
Spearman’s method describes the degree of correlation between the <em>ranks</em>, not
the actual value. This is one drawback of choosing Spearman’s instead of
Pearson’s correlation.</p>
</div>
</div>
<div id="kendalls-tau" class="section level1">
<h1>Kendall’s <span class="math inline">\(\tau\)</span></h1>
<p>Kendall’s <span class="math inline">\(\tau\)</span> is another non-parametric correlation test. However, a bit
different from Spearman’s and Pearson’s, Kendall’s method measure the degree of
concordance between two variables. Kendall’s <span class="math inline">\(\tau\)</span> is most suitable to handle
an ordinal data, where its variant provides a good measure on data with tied
values. Kendall’s <span class="math inline">\(\tau\)</span> presents with three methods of <span class="math inline">\(\tau_A\)</span>, <span class="math inline">\(\tau_B\)</span> and
<span class="math inline">\(\tau_C\)</span>. The preference of choosing which method to employ should align with
the shape of our data. Both of <span class="math inline">\(\tau_A\)</span> and <span class="math inline">\(\tau_B\)</span> are suitable to handle
ordinal data with the same measurement scale, e.g. both are using 10-point
Likert scale. While in <span class="math inline">\(\tau_C\)</span>, we can use two ordinal variables with
different measurement scale. It is easier to understand this concept by
examining the equation. However, before that, it is important to understand
what we mean by concordant and discordant data.</p>
<p>For <span class="math inline">\(i, j \in X, Y: i \neq j,\ \exists\ (x_{i, j}, y_{i, j})\)</span>. For each pair of
<span class="math inline">\((x_{i, j}, y_{i, j})\)</span>, we regard concordance as <span class="math inline">\((x_i &lt; x_j \ \texttt{and}\
y_i &lt; y_j) \lor (x_i &gt; x_j \ \texttt{and}\  y_i &gt; y_j)\)</span>, or otherwise a
discordance. In other words, we have a concordant pair when we observe the same
relationship for index <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in both variables.</p>
<p><span class="math display">\[\begin{align}
\tau_a &amp;= \frac{n_c - n_d}{n}\\
\tau_b &amp;= \frac{n_c - n_d}{\sqrt{(n + X_0) (n + Y_0)}} \\
\tau_c &amp;= \frac{2(n_c - n_d)}{n^2 \frac{(m-1)}{m}} \\
n      &amp;= \binom{n}{2}
\end{align}\]</span></p>
<p>Here, <span class="math inline">\(n_c\)</span> and <span class="math inline">\(n_d\)</span> are the numbers of concordant and discordant data,
respectively. In the second equation of <span class="math inline">\(\tau_B\)</span>, we also count for <span class="math inline">\(X_0 and Y_0\)</span>, which is the number of ties. In <span class="math inline">\(\tau_C\)</span>, we have <span class="math inline">\(m\)</span> which reflects the
minimum number of row and column between two different measurement scales. In
brief, we know how to use each method and we can conclude that:</p>
<ul>
<li><span class="math inline">\(\tau_a\)</span>: Square table (same measurement scale)</li>
<li><span class="math inline">\(\tau_b\)</span>: Square table, handles tie</li>
<li><span class="math inline">\(\tau_c\)</span>: Rectangular table (different scale), handles tie</li>
</ul>
<div id="example-please-3" class="section level2">
<h2>Example, please?</h2>
<p>Again, this example will use the same data subset.</p>
<pre class="r"><code>tbl &lt;- subset(iris, select=-Species) %T&gt;% str()</code></pre>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  4 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...</code></pre>
<pre class="r"><code>cov(tbl)</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length        0.686      -0.042         1.27        0.52
## Sepal.Width        -0.042       0.190        -0.33       -0.12
## Petal.Length        1.274      -0.330         3.12        1.30
## Petal.Width         0.516      -0.122         1.30        0.58</code></pre>
<pre class="r"><code>cor.test(tbl$Sepal.Length, tbl$Sepal.Width, method=&quot;kendall&quot;)</code></pre>
<pre><code>## 
##  Kendall&#39;s rank correlation tau
## 
## data:  tbl$Sepal.Length and tbl$Sepal.Width
## z = -1, p-value = 0.2
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##    tau 
## -0.077</code></pre>
<p>The value of <span class="math inline">\(\tau\)</span> is in the range of <span class="math inline">\([0, 1]\)</span>, with any sign of <span class="math inline">\(\tau &lt; 0\)</span> being an
artefact. So, when interpreting the <span class="math inline">\(\tau\)</span>, we will only take the absolute
value, <span class="math inline">\(|\tau|\)</span>. The base installation of <code>R</code> only implements <span class="math inline">\(\tau_A\)</span>
calculation. If needed, other packages are available to provide a different
Kendall’s approach.</p>
</div>
</div>
<div id="recap" class="section level1">
<h1>Recap</h1>
<ul>
<li>Check normality</li>
<li>Check linearity</li>
<li>Non-parametric test: determine the presence of tie</li>
<li>Perform correlation</li>
<li>Create the plot (if necessary)</li>
</ul>
</div>
<div id="caveats" class="section level1">
<h1>Caveats</h1>
<ul>
<li>We only discussed <em>some</em> of the popular correlation test</li>
<li>All discussed methods assume I.I.D</li>
<li>Paired data is suitable for none of discussed methods</li>
<li>Time series data requires a different approach</li>
<li>Correlation <span class="math inline">\({\neq}\)</span> Causation</li>
</ul>
</div>
<div id="more-on-correlations" class="section level1">
<h1>More on Correlations</h1>
<ul>
<li>Concordance correlation coefficient</li>
<li>Intraclass correlation</li>
<li>Partial correlation</li>
<li>Zero-order correlation</li>
<li>The list goes on…</li>
</ul>
</div>
