---
author: lam
title: "Non-parametric: Differences in Two Groups"
weight: 8
description: >

  A parametric test requires us to assume or hypothesize parameters in a
  population. Often, a small sample size or a highly-skewed distribution does not
  resemble a normal distribution. In such a case, it becomes impertinent to
  assume normality in our data. Even though parametric tests are quite robust
  against non-normal data to a certain degree, it still requires a large number
  of sample. With larger $n$ and homogeneous intergroup variance, the parametric
  test may have a sufficient power to correctly reject the $H_0$. However, if we
  cannot satisfy the required assumption, we need to drop our hypothesized claim
  of population parameters. In other words, we are employing a non-parametric
  test to measure observed differences.

summary: >

  A parametric test requires us to assume or hypothesize parameters in a
  population. Often, a small sample size or a highly-skewed distribution does not
  resemble a normal distribution. In such a case, it becomes impertinent to
  assume normality in our data. Even though parametric tests are quite robust
  against non-normal data to a certain degree, it still requires a large number
  of sample. With larger $n$ and homogeneous intergroup variance, the parametric
  test may have a sufficient power to correctly reject the $H_0$. However, if we
  cannot satisfy the required assumption, we need to drop our hypothesized claim
  of population parameters. In other words, we are employing a non-parametric
  test to measure observed differences.

date: 2020-11-15
categories: ["statistics", "ukrida"]
tags: ["R", "hypothesis"]
slug: 08-nonpar-meandiff
csl: ../harvard.csl
bibliography: ../ref.bib
draft: false
---

<script src="{{< blogdown/postref >}}index_files/kePrint/kePrint.js"></script>
<link href="{{< blogdown/postref >}}index_files/lightable/lightable.css" rel="stylesheet" />


<p><a href="https://lamurian.rbind.io/note/biostat-ukrida/08-nonpar-meandiff/slide">Slide</a></p>
<p>A parametric test requires us to assume or hypothesize parameters in a
population. Often, a small sample size or a highly-skewed distribution does not
resemble a normal distribution. In such a case, it becomes impertinent to
assume normality in our data. Even though parametric tests are quite robust
against non-normal data to a certain degree, it still requires a large number
of sample. With larger <span class="math inline">\(n\)</span> and homogeneous intergroup variance, the parametric
test may have a sufficient power to correctly reject the <span class="math inline">\(H_0\)</span>. However, if we
cannot satisfy the required assumption, we need to drop our hypothesized claim
of population parameters. In other words, we are employing a non-parametric
test to measure observed differences.</p>
<div id="non-parametric-test" class="section level1">
<h1>Non-Parametric Test</h1>
<p>Given a small sample size, it is difficult for us to ascertain normality. As we
have previously discussed, a large sample size merits a high statistical power.
Conversely, when we have a low sample size, it is just natural to expect a
lower statistical power. Asides from normality assumption, parametric tests are
also sensitive to a severe skewness, because the average value will not
represent the central tendency. Non-parametric test neither assume normality
nor symmetricity, thus providing a legible approach for data unsuitable for
parametric tests. However, having a more lenient assumption means to neglect
some information in the data, resulting in a lower statistical power compared to
the parametric test given its assumptions fulfilled. Following figures
represent how the <span class="math inline">\(\mu\)</span> not describing the central tendency in a skewed data
(bottom).</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.data-1.png" width="100%" /></p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.data.skew-1.png" width="100%" /></p>
<p>As a summary, we should consider using a non-parametric test in the case of
having a small sample size. If our data is not asymptotically normal, then
employing a non-parametric test might be a more appropriate step. The presence
of extreme outliers or severe skewness may impair the parametric test, so using
a non-parametric test is desirable. In conducting a non-parametric test, we
hypothesize on the difference in our observation compared to its reference. To
describe the difference, we shall use its median value <span class="math inline">\(M\)</span>, therefore:</p>
<ul>
<li><span class="math inline">\(H_0:\ M_1 = M_2\)</span></li>
<li><span class="math inline">\(H_1:\ M_1 \neq M_2\)</span></li>
</ul>
</div>
<div id="one-sample-test" class="section level1">
<h1>One-Sample Test</h1>
<p>Similar to the parametric test, in one-sample test we only have one group of
observation. We would like to know whether our group deviates from the
hypothesized median. We may employ two type of test, i.e. a one-sample sign
test and one-sample Wilcoxon signed rank test. Only the one-sample Wilcoxon
test is analogous to the one-sample T-Test</p>
<div id="one-sample-sign-test" class="section level2">
<h2>One-Sample Sign Test</h2>
<p>A one-sample sign test does not assume normality nor symmetric distribution. It
is useful in a skewed data, where the statistics follow a binomial
distribution. In fact, it is an extension to the binomial test, which we have
discussed in the <a href="https://lamurian.rbind.io/note/biostat-ukrida/05-independence/">previous lecture</a>.
In this test, we know that our statistics <span class="math inline">\(B_s \sim B(n, p)\)</span> with <span class="math inline">\(0=0.5\)</span>. We
set the probability <span class="math inline">\(p=0.5\)</span> because the random chance of having <span class="math inline">\(M_1 \neq M_0\)</span>
is 0.5, as the median is the midpoint.</p>
<p>To calculate one-sample sign test, we need to:</p>
<ul>
<li>Find the residual between our observation and hypothesized median</li>
<li>Omit all 0</li>
<li>Disregard the magnitude, take only its sign</li>
<li>Calculate the frequency of positive and negative signs</li>
<li>Let <span class="math inline">\(B_s\)</span> be the resultant <span class="math inline">\(\to B_s \sim B(n, 0.5)\)</span></li>
</ul>
<p>Essentially, we only have two outcome of interest, i.e. having a positive or
negative sign. Assuming I.I.D in each of our observation, we have a Bernoulli
trial with <span class="math inline">\(p=0.5\)</span>, such that we can model our probability as a Binomial
distribution.</p>
<div id="example-please" class="section level3">
<h3>Example, please?</h3>
<pre class="r"><code># Generate a skewed data using a Chi-squared distribution
set.seed(1)
x &lt;- rchisq(10, 4) %T&gt;% print()</code></pre>
<pre><code>##  [1] 1.66 7.14 6.93 4.10 7.77 5.08 4.58 2.30 1.36 1.67</code></pre>
<p>In this example, we have <span class="math inline">\(X \sim \chi^2(4)\)</span>, presenting as a skewed data.
Should we let <span class="math inline">\(H_0\)</span> be <span class="math inline">\(M=5\)</span> and we are interested to conduct a two-tailed
test, we can proceed using a one-sample sign test.</p>
<pre class="r"><code># Set M and find the residual (difference)
M &lt;- 5
diff &lt;- {x - M}

# Make a data frame
tbl &lt;- data.frame(x=x, abs.diff=abs(diff), sign=sign(diff))</code></pre>
<table>
<thead>
<tr>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
abs.diff
</th>
<th style="text-align:right;">
sign
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1.66
</td>
<td style="text-align:right;">
3.338
</td>
<td style="text-align:right;">
-1
</td>
</tr>
<tr>
<td style="text-align:right;">
7.14
</td>
<td style="text-align:right;">
2.142
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
6.93
</td>
<td style="text-align:right;">
1.926
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
4.10
</td>
<td style="text-align:right;">
0.898
</td>
<td style="text-align:right;">
-1
</td>
</tr>
<tr>
<td style="text-align:right;">
7.77
</td>
<td style="text-align:right;">
2.771
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
5.08
</td>
<td style="text-align:right;">
0.081
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
4.58
</td>
<td style="text-align:right;">
0.424
</td>
<td style="text-align:right;">
-1
</td>
</tr>
<tr>
<td style="text-align:right;">
2.30
</td>
<td style="text-align:right;">
2.701
</td>
<td style="text-align:right;">
-1
</td>
</tr>
<tr>
<td style="text-align:right;">
1.36
</td>
<td style="text-align:right;">
3.638
</td>
<td style="text-align:right;">
-1
</td>
</tr>
<tr>
<td style="text-align:right;">
1.67
</td>
<td style="text-align:right;">
3.329
</td>
<td style="text-align:right;">
-1
</td>
</tr>
</tbody>
</table>
<pre class="r"><code># Perform a binomial test
res &lt;- lapply(c(-1, 1), function(sign) {
    binom.test(sum(tbl$sign==sign), nrow(tbl), 0.5) %&gt;%
        broom::tidy()
})

# Two-tailed test on sign=-1
knitr::kable(res[[1]])</code></pre>
<table>
<thead>
<tr>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:right;">
parameter
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:left;">
method
</th>
<th style="text-align:left;">
alternative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0.754
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.262
</td>
<td style="text-align:right;">
0.878
</td>
<td style="text-align:left;">
Exact binomial test
</td>
<td style="text-align:left;">
two.sided
</td>
</tr>
</tbody>
</table>
<pre class="r"><code># Two-tailed test on sign=1
knitr::kable(res[[2]])</code></pre>
<table>
<thead>
<tr>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:right;">
parameter
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:left;">
method
</th>
<th style="text-align:left;">
alternative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.754
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.122
</td>
<td style="text-align:right;">
0.738
</td>
<td style="text-align:left;">
Exact binomial test
</td>
<td style="text-align:left;">
two.sided
</td>
</tr>
</tbody>
</table>
<p>As we have demonstrated, a non-parametric test is straightforward and simple.
Statistics obtained from a one-sample sign test only taking into account the
difference and neglecting the magnitude. This test is suitable to avoid
incorrectly rejecting the <span class="math inline">\(H_0\)</span> due to biased calculation in the presence of
severe skewness.</p>
</div>
</div>
<div id="one-sample-wilcoxon-signed-rank-test" class="section level2">
<h2>One-Sample Wilcoxon Signed Rank Test</h2>
<p>Similar to the one-sample sign test, one-sample Wilcoxon test also does not
assume normality. Contrarily, it assumes a symmetric distribution because
one-sample Wilcoxon test is not suitable for a severely-skewed data. We may
start to wonder, what distribution has a symmetric shape but not normal? Well,
that is a fair question, because so far we rarely discuss about non-normally
distributed symmetric data. Though, we have briefly mentioned one of them in
the second lecture, the uniform distribution. Of course, there are more
examples such as Cauchy distribution, a generalized normal distribution (which
is not normal!), and so on. We will not dig too much into this topic, but
please be aware that we may have a symmetric but non-normally distributed data.</p>
<p>One-sample Wilcoxon test has a similar procedure to one-sample sign test.
However, we assign ranks based on computed differences. The statistics is the
resultants of signed rank, where we only consider the minimum value between
both the sum of positive and negative ranks. Thus, we sometimes refer
one-sample Wilcoxon as a sum rank signed test.</p>
<div id="example-please-1" class="section level3">
<h3>Example, please?</h3>
<p>To keep a consistent remark, we shall re-use the previous data. Please be
advised, we need to ascertain symmetricity before proceeding with a one-sample
Wilcoxon test. To do so, we may refer to a skewness measure of our data. When
we have a negative value, it means our data is left skewed, <em>vice versa</em>.
Acquiring the skewness, we may use the range of <span class="math inline">\([-1, 1]\)</span> to decide that our
data does not present with a severe skewness impairing its symmetricity.</p>
<pre class="r"><code># Generate a skewed data using a Chi-squared distribution
set.seed(1)
x &lt;- rchisq(10, 4) %T&gt;% print()</code></pre>
<pre><code>##  [1] 1.66 7.14 6.93 4.10 7.77 5.08 4.58 2.30 1.36 1.67</code></pre>
<pre class="r"><code># Add columns to data frame
tbl$ranked &lt;- rank(tbl$abs.diff)</code></pre>
<table>
<thead>
<tr>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
abs.diff
</th>
<th style="text-align:right;">
sign
</th>
<th style="text-align:right;">
ranked
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1.66
</td>
<td style="text-align:right;">
3.338
</td>
<td style="text-align:right;">
-1
</td>
<td style="text-align:right;">
9
</td>
</tr>
<tr>
<td style="text-align:right;">
7.14
</td>
<td style="text-align:right;">
2.142
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:right;">
6.93
</td>
<td style="text-align:right;">
1.926
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
4.10
</td>
<td style="text-align:right;">
0.898
</td>
<td style="text-align:right;">
-1
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
7.77
</td>
<td style="text-align:right;">
2.771
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
7
</td>
</tr>
<tr>
<td style="text-align:right;">
5.08
</td>
<td style="text-align:right;">
0.081
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
4.58
</td>
<td style="text-align:right;">
0.424
</td>
<td style="text-align:right;">
-1
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
2.30
</td>
<td style="text-align:right;">
2.701
</td>
<td style="text-align:right;">
-1
</td>
<td style="text-align:right;">
6
</td>
</tr>
<tr>
<td style="text-align:right;">
1.36
</td>
<td style="text-align:right;">
3.638
</td>
<td style="text-align:right;">
-1
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:right;">
1.67
</td>
<td style="text-align:right;">
3.329
</td>
<td style="text-align:right;">
-1
</td>
<td style="text-align:right;">
8
</td>
</tr>
</tbody>
</table>
<pre class="r"><code># Calculate the statistics
W &lt;- tapply(tbl$ranked, tbl$sign, sum) %&gt;% min() %T&gt;% print()</code></pre>
<pre><code>## [1] 17</code></pre>
<pre class="r"><code># Find the p-value for a two-tailed test
psignrank(W, nrow(tbl)) * 2</code></pre>
<pre><code>## [1] 0.322</code></pre>
<pre class="r"><code># Built-in test
wilcox.test(x, data=tbl, mu=5)</code></pre>
<pre><code>## 
##  Wilcoxon signed rank exact test
## 
## data:  x
## V = 17, p-value = 0.3
## alternative hypothesis: true location is not equal to 5</code></pre>
</div>
</div>
</div>
<div id="two-sample-test" class="section level1">
<h1>Two-Sample Test</h1>
<p>Mann-Whitney U test is an unpaired two-sample Wilcoxon test. As in other
non-parametric tests, it does not assume normality, although it requires the
data be I.I.D. Mann-Whitney U test can also handle skewed data with a small
sample size. The concept in Mann-Whitney U test is the sum of ranks, where we
pooled all the data elements from both groups. Then we sorted pooled values,
starting from the smallest to largest. Similar to one-sample Wilcoxon test, we
have to assign a rank to each value in order to compare both groups.</p>
<div id="example-please-2" class="section level2">
<h2>Example, please?</h2>
<pre class="r"><code># We will use x as the first group
x</code></pre>
<pre><code>##  [1] 1.66 7.14 6.93 4.10 7.77 5.08 4.58 2.30 1.36 1.67</code></pre>
<pre class="r"><code># Assign x+4 as the second group, make a data frame
tbl &lt;- data.frame(
    obs=c(x, x+4), 
    group=rep(c(&quot;1&quot;, &quot;2&quot;), each=length(x)) %&gt;% factor()
) %T&gt;% str()</code></pre>
<pre><code>## &#39;data.frame&#39;:    20 obs. of  2 variables:
##  $ obs  : num  1.66 7.14 6.93 4.1 7.77 ...
##  $ group: Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<pre class="r"><code># Goodness of fit test to determine the distribution
tapply(tbl$obs, tbl$group, ks.test, pnorm) %&gt;% lapply(broom::tidy) %&gt;%
    lapply(data.frame) %&gt;% {do.call(rbind, .)} %&gt;% kable() %&gt;% kable_minimal()</code></pre>
<table class=" lightable-minimal" style="font-family: &quot;Trebuchet MS&quot;, verdana, sans-serif; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:left;">
method
</th>
<th style="text-align:left;">
alternative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.913
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
One-sample Kolmogorov-Smirnov test
</td>
<td style="text-align:left;">
two-sided
</td>
</tr>
<tr>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
One-sample Kolmogorov-Smirnov test
</td>
<td style="text-align:left;">
two-sided
</td>
</tr>
</tbody>
</table>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.tbl-1.png" width="100%" /></p>
<p>As we can see, both data presented with a non-normal distribution. Thus,
comparing both observations will require a non-parametric test. Here we employ
Mann-Whitney U test and compute the effect size.</p>
<pre class="r"><code>wilcox.test(obs ~ group, data=tbl, conf.int=TRUE)</code></pre>
<pre><code>## 
##  Wilcoxon rank sum exact test
## 
## data:  obs by group
## W = 12, p-value = 0.003
## alternative hypothesis: true location shift is not equal to 0
## 95 percent confidence interval:
##  -6.74 -1.26
## sample estimates:
## difference in location 
##                     -4</code></pre>
<pre class="r"><code>rstatix::wilcox_effsize(obs ~ group, data=tbl)</code></pre>
<pre><code>## # A tibble: 1 × 7
##   .y.   group1 group2 effsize    n1    n2 magnitude
## * &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    
## 1 obs   1      2        0.642    10    10 large</code></pre>
</div>
</div>
<div id="paired-test" class="section level1">
<h1>Paired Test</h1>
<p>In previous tests, we always assume I.I.D. In case of paired data, i.e. when an
observation influencing another, we need to take into consideration their
relationship. Unfortunately, Mann-Whitney U could not discern such a relation
between data point, as it only looks at the difference in rank. In paired
Wilcoxon test, we do not assume I.I.D., and procedure-wise it is akin to
one-sample Wilcoxon test. To understand this concept, please kindly recall how
paired T-Test relates to one-sample T-Test. Although paired Wilcoxon test does
not assume the shape of our distribution, a symmetric data is still a plus.</p>
<p>To conduct a paired Wilcoxon test, we first measure the difference between
paired data points by taking their residuals. Similar observations will result
in 0 residual, which we have to omit out of our observation. We assign rank to
the absolute value of computed residual then calculate the statistics as we
previously demonstrated in the one-sample Wilcoxon test.</p>
<div id="example-please-3" class="section level2">
<h2>Example, please?</h2>
<p>In the previous lecture about ANOVA, we are using <code>ChickWeight</code> dataset. We
happened to observe a non-normally distributed data for two group of times.
Here, we will further examine both groups to understand the difference.</p>
<pre class="r"><code># We will use the ChickWeight dataset
str(ChickWeight)</code></pre>
<pre><code>## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;:   578 obs. of  4 variables:
##  $ weight: num  42 51 59 64 76 93 106 125 149 171 ...
##  $ Time  : num  0 2 4 6 8 10 12 14 16 18 ...
##  $ Chick : Ord.factor w/ 50 levels &quot;18&quot;&lt;&quot;16&quot;&lt;&quot;15&quot;&lt;..: 15 15 15 15 15 15 15 15 15 15 ...
##  $ Diet  : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  - attr(*, &quot;formula&quot;)=Class &#39;formula&#39;  language weight ~ Time | Chick
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##  - attr(*, &quot;outer&quot;)=Class &#39;formula&#39;  language ~Diet
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##  - attr(*, &quot;labels&quot;)=List of 2
##   ..$ x: chr &quot;Time&quot;
##   ..$ y: chr &quot;Body weight&quot;
##  - attr(*, &quot;units&quot;)=List of 2
##   ..$ x: chr &quot;(days)&quot;
##   ..$ y: chr &quot;(gm)&quot;</code></pre>
<pre class="r"><code># Assess normality
tapply(ChickWeight$weight, ChickWeight$Time, shapiro.test) %&gt;% lapply(broom::tidy) %&gt;%
    lapply(data.frame) %&gt;% {do.call(rbind, .)} %&gt;% knitr::kable()</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:left;">
method
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:right;">
0.890
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
2
</td>
<td style="text-align:right;">
0.873
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
4
</td>
<td style="text-align:right;">
0.973
</td>
<td style="text-align:right;">
0.315
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
6
</td>
<td style="text-align:right;">
0.982
</td>
<td style="text-align:right;">
0.648
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
8
</td>
<td style="text-align:right;">
0.980
</td>
<td style="text-align:right;">
0.577
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
10
</td>
<td style="text-align:right;">
0.981
</td>
<td style="text-align:right;">
0.616
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
12
</td>
<td style="text-align:right;">
0.983
</td>
<td style="text-align:right;">
0.686
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
14
</td>
<td style="text-align:right;">
0.973
</td>
<td style="text-align:right;">
0.325
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
16
</td>
<td style="text-align:right;">
0.986
</td>
<td style="text-align:right;">
0.830
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
18
</td>
<td style="text-align:right;">
0.991
</td>
<td style="text-align:right;">
0.975
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
20
</td>
<td style="text-align:right;">
0.991
</td>
<td style="text-align:right;">
0.968
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
21
</td>
<td style="text-align:right;">
0.986
</td>
<td style="text-align:right;">
0.869
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
</tbody>
</table>
<p>After assessing normality, we see that both <span class="math inline">\(T=0\)</span> and <span class="math inline">\(T=2\)</span> not to follow a
normal distribution.</p>
<pre class="r"><code># Subset the dataset to exclude normally distributed data
tbl &lt;- subset(ChickWeight, subset={ChickWeight$Time %in% c(0, 2)})

# Make Time as a factor
tbl$Time %&lt;&gt;% factor(levels=c(0, 2))</code></pre>
<p>Since we are interested to observe differences in non-normal distribution, here
we subset the data to only include observation in group <span class="math inline">\(T_0\)</span> and <span class="math inline">\(T_2\)</span>. Then
we turn our variables into a factor, where we set <span class="math inline">\(T_0\)</span> as our group of
reference.</p>
<pre class="r"><code># Perform a paired Wilcoxon test
wilcox.test(weight ~ Time, data=tbl, paired=TRUE, conf.int=TRUE)</code></pre>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  weight by Time
## V = 8, p-value = 1e-09
## alternative hypothesis: true location shift is not equal to 0
## 95 percent confidence interval:
##  -9.0 -7.5
## sample estimates:
## (pseudo)median 
##           -8.5</code></pre>
<pre class="r"><code>rstatix::wilcox_effsize(weight ~ Time, data=tbl, paired=TRUE)</code></pre>
<pre><code>## # A tibble: 1 × 7
##   .y.    group1 group2 effsize    n1    n2 magnitude
## * &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    
## 1 weight 0      2        0.862    50    50 large</code></pre>
<p>Lastly, we can perform a paired Wilcoxon test and its associated effect size.</p>
</div>
</div>
