---
author: lam
title: "Hypothesis Test: Proportional Difference"
weight: 5
description: >

  In statistics, proving or rejecting an assumption requires a rigorous formal
  approach, which highly depends on the formulated hypothesis. The current post
  will help us in conducting a statistical test to measure hypothesized
  proportional difference among categorical variables. Another common name
  assigned to such a formal step is the test of independence. We shall see
  various tests we have, how they work and when to use them.

summary: >

  In statistics, proving or rejecting an assumption requires a rigorous formal
  approach, which highly depends on the formulated hypothesis. The current post
  will help us in conducting a statistical test to measure hypothesized
  proportional difference among categorical variables. Another common name
  assigned to such a formal step is the test of independence. We shall see
  various tests we have, how they work and when to use them.

date: 2020-10-09
categories: ["statistics", "ukrida"]
tags: ["R", "hypothesis"]
slug: 05-independence
csl: ../harvard.csl
bibliography: ../ref.bib
draft: false
---

<script src="{{< blogdown/postref >}}index_files/kePrint/kePrint.js"></script>
<link href="{{< blogdown/postref >}}index_files/lightable/lightable.css" rel="stylesheet" />


<p><a href="https://lamurian.rbind.io/note/biostat-ukrida/05-independence/slide">Slide</a></p>
<p>In statistics, proving or rejecting an assumption requires a rigorous formal
approach, which highly depends on the formulated hypothesis. The current post
will help us in conducting a statistical test to measure hypothesized
proportional difference among categorical variables. Another common name
assigned to such a formal step is the test of independence. We shall see
various tests we have, how they work and when to use them.</p>
<div id="proportional-difference" class="section level1">
<h1>Proportional Difference</h1>
<p>We may recall from the first lecture that we can measure proportion in a
population or sample as a ratio of an event to the total space. As an example,
imagine we have 100 participants in a survey, where 60 involved respondents are male
and the rest are female. We can calculate the proportion of male participants
as <span class="math inline">\(\frac{60}{100} = 0.6\)</span>. In previous lectures, we also performed a number of
experiments involving coin flipping and calculating the conditional probability
of <span class="math inline">\(P(X=x|n, p)\)</span>, by considering <span class="math inline">\(X \sim B(n, p)\)</span>. What we previously did was
proving a <span class="math inline">\(H_0: \hat{p} = p\)</span>, with <span class="math inline">\(\hat{p}\)</span> being the proportion seen in our
sample and <span class="math inline">\(p\)</span> is the assumed natural occurrence of probability in a fair coin,
0.5. We have yet to coin the term yet (pun intended), but we have grown
familiar with a form of hypothesis testing using a binomial test.</p>
<p>Binomial test is a form of exact hypothesis test for one group proportion. With
an exact test, we can expect the computed probability of proposed <span class="math inline">\(H_0\)</span> being
true reflects the actual probability. In later sections, we shall observe how
calculating an exact probability gets more tedious with larger sample size and
more complex comparison. As a solution, we often rely on an approximation, e.g.
using proportion test for one group proportion problem as an alternative to
binomial test. As we have gotten quite comfortable with the idea of flipping a
coin, we may use this example as a proof of concept in distinguishing an exact
test from its approximation.</p>
<p><span class="math display">\[Let X \sim B(n, p)\]</span>
<span class="math display">\[\texttt{Test the probability of having: } P(X=6 \ |\ 10, 0.5)\]</span></p>
<p><span class="math display">\[\begin{align}
H_0 &amp;: P(X=6) = 0.5 \\
H_a &amp;: P(X=6) \neq 0.5
\end{align}\]</span></p>
<table>
<caption>
(#tab:prop.1)Binomial test
</caption>
<thead>
<tr>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:right;">
parameter
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:left;">
method
</th>
<th style="text-align:left;">
alternative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0.754
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.262
</td>
<td style="text-align:right;">
0.878
</td>
<td style="text-align:left;">
Exact binomial test
</td>
<td style="text-align:left;">
two.sided
</td>
</tr>
</tbody>
</table>
<table>
<caption>
(#tab:prop.1)Proportion test
</caption>
<thead>
<tr>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:right;">
parameter
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:left;">
method
</th>
<th style="text-align:left;">
alternative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.752
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.274
</td>
<td style="text-align:right;">
0.863
</td>
<td style="text-align:left;">
1-sample proportions test with continuity correction
</td>
<td style="text-align:left;">
two.sided
</td>
</tr>
</tbody>
</table>
<p>Using an approximation, the proportion test gives a slightly different results
compared to the binomial test (hint: look at the p-value and confidence
interval). Now we may perform the same procedure, but using a higher sample
space, where we indicate the problem as follows:</p>
<p><span class="math display">\[Let X \sim B(n, p)\]</span>
<span class="math display">\[\texttt{Test the probability of having: } P(X=60 \ |\ 100, 0.5)\]</span></p>
<p><span class="math display">\[\begin{align}
H_0 &amp;: P(X=60) = 0.5 \\
H_a &amp;: P(X=60) \neq 0.5
\end{align}\]</span></p>
<table>
<caption>
(#tab:prop.2)Binomial test
</caption>
<thead>
<tr>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:right;">
parameter
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:left;">
method
</th>
<th style="text-align:left;">
alternative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
0.057
</td>
<td style="text-align:right;">
100
</td>
<td style="text-align:right;">
0.497
</td>
<td style="text-align:right;">
0.697
</td>
<td style="text-align:left;">
Exact binomial test
</td>
<td style="text-align:left;">
two.sided
</td>
</tr>
</tbody>
</table>
<table>
<caption>
(#tab:prop.2)Proportion test
</caption>
<thead>
<tr>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:right;">
parameter
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:left;">
method
</th>
<th style="text-align:left;">
alternative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
3.61
</td>
<td style="text-align:right;">
0.057
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.497
</td>
<td style="text-align:right;">
0.695
</td>
<td style="text-align:left;">
1-sample proportions test with continuity correction
</td>
<td style="text-align:left;">
two.sided
</td>
</tr>
</tbody>
</table>
<p>With a larger sample size, the proportion test gives a better estimation of the
binomial test. Notice how the p-value and confidence interval produced by the
proportion test gets closer to the binomial test, as compared to our previous
test. In fact, with sample size of <span class="math inline">\(n \to \infty\)</span>, we can expect a closer
approximation to the exact test. The perk of using approximation is its
comparably lower computational power and less stringent assumptions.</p>
<p>However, we are often curious to observe multiple variables, i.e. a
proportional difference in multiple groups. In such cases, neither of binomial
nor proportion test can help us! To identify the problem, first we need to
visualize each observation into a contingency table. Then we may apply a more
appropriate test, e.g. with Fisher’s exact test or an approximation using
Pearson’s Chi-square.</p>
<p>Contingency table is a table with <span class="math inline">\(m \times n\)</span> cells, usually takes form as a
<span class="math inline">\(2 \times 2\)</span> table. The row and column of a contingency table represents two
variables of interest, respectively. With an arbitrary element, we can draw a
contingency table as follows:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Outcome 1
</th>
<th style="text-align:left;">
Outcome 2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Exposure 1
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:left;">
b
</td>
</tr>
<tr>
<td style="text-align:left;">
Exposure 2
</td>
<td style="text-align:left;">
c
</td>
<td style="text-align:left;">
d
</td>
</tr>
</tbody>
</table>
<p>To help us visualize a contingency table, suppose we are conducting a market
research in Jakarta, where we aim to see how people express their preferences
in choosing chain store outlets. We categorized participants based on their
place of residency, i.e. in suburban and urban area. The mini-market chain of
our interest would be Indomaret and Alfamart. We observed 30 out of 50
respondents in suburban area choose Indomaret, compared to 20 out of 50
respondents in urban area.</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Indomaret
</th>
<th style="text-align:right;">
Alfamart
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Suburban
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
20
</td>
</tr>
<tr>
<td style="text-align:left;">
Urban
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
30
</td>
</tr>
</tbody>
</table>
<p>As a test of proportional difference, both Fisher’s exact test and Pearson’s
Chi-square can aid us in inferring current data on our market research. On
later section we will see their limitation and what use cases are available.</p>
</div>
<div id="fishers-exact-test" class="section level1">
<h1>Fisher’s Exact Test</h1>
<p>Fisher’s test provides an exact p-value calculation, where it follows a
hypergeometric distribution. From the previous lecture, we have learnt what
geometric distribution is, i.e. a specific form of binomial distribution, where
we are interested to calculate the probability of having one event in <span class="math inline">\(n\)</span>
number of trials. The hypergeometric distribution is somewhat similar to the
binomial distribution, with each instance not being identical. In the binomial
distribution, we can expect each trial following a Bernoulli trial with
identical probability of success. A hypergeometric distribution assumes an
event with replacement (kindly recall probability concepts from the urn
problem). In other word:</p>
<ul>
<li>Binomial distribution solves the probability of having <span class="math inline">\(k\)</span> successes
within <span class="math inline">\(n\)</span> number of trials without replacement</li>
<li>Geometry distribution finds the probability of having 1 success within <span class="math inline">\(n\)</span>
number of trials without replacement</li>
<li>Hypergeometry distribution looks for the probability of having <span class="math inline">\(k\)</span> successes
within <span class="math inline">\(n\)</span> number of trials <em>with</em> replacement</li>
</ul>
<p>Looking at proportional differences in general, we can formulate our
hypotheses as follow:
<span class="math display">\[\begin{align}
H_0 &amp;: \hat{p_1} = \hat{p_2} \\
H_a &amp;: \hat{p_1} \neq \hat{p_2}
\end{align}\]</span>
with <span class="math inline">\(\hat{p_i}\)</span> being the proportion in group <span class="math inline">\(i\)</span>.</p>
<p>Since Fisher’s method calculate the <em>exact</em> p-value, we can solve the
probability as a permutation problem. Therefore, we can calculate the
probability of <em>each</em> event using one of following equations:</p>
<p><span class="math display">\[\begin{align}
P &amp;= \frac{\binom{a + b}{a} \binom{a + b}{b}} {\binom{n}{a + b}} \tag{1} \\
  \\
  &amp;= \frac{\binom{c + d}{c} \binom{c + d}{d}} {\binom{n}{c + d}} \tag{2} \\
  \\
  &amp;= \frac{(a+b)!\ (c+d)!\ (a+c)!\ (b+d)!} {a!\ b!\ c!\ d!\ n!} \tag{3} \\
  \\
  \\
  \\
n &amp;= a + b + c + d
\end{align}\]</span></p>
<p>Seeing the mathematical equation may not sound too appealing for some people,
we can also simplify equation 1 into a code, as follow:</p>
<pre class="r"><code>fisher.eq &lt;- function(abcd) { # abcd is a list of 4 elements
    a &lt;- abcd[1]; b &lt;- abcd[2]; c &lt;- abcd[3]; d &lt;- abcd[4]
    choose(a+b, a) * choose(a+b, b) / choose(a+b+c+d, a+b)
}</code></pre>
<p>Beware though, as Fisher’s test being an exact approach, we need to calculate
<em>all</em> possible extreme events to obtain the p-value. In our case, we have <span class="math inline">\(a, b, c, d\)</span> as an array of <span class="math inline">\((30,20,20,30)\)</span>, so we need to address <strong>all</strong>
extreme events which satisfy <span class="math inline">\((a,b,c,d) \in \{(31,19,19,31),...,(50,0,0,50)\}\)</span>.</p>
<table>
<thead>
<tr>
<th style="text-align:right;">
a
</th>
<th style="text-align:right;">
b
</th>
<th style="text-align:right;">
c
</th>
<th style="text-align:right;">
d
</th>
<th style="text-align:right;">
probability
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
0.022
</td>
</tr>
<tr>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
0.009
</td>
</tr>
<tr>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
0.003
</td>
</tr>
<tr>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
0.001
</td>
</tr>
<tr>
<td style="text-align:right;">
34
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
37
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
37
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
39
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
39
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
41
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
42
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
42
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
46
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
46
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
49
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
49
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
</tbody>
</table>
<p>To calculate p-value in a one-tailed test, we simply sum all the probabilities:</p>
<pre class="r"><code>sum(tbl$probability)</code></pre>
<pre><code>## [1] 0.0357</code></pre>
<p>Having the product of one-tailed p-value multiplied by <span class="math inline">\(2\)</span>, we can get the
two-tailed p-value:</p>
<pre class="r"><code>sum(tbl$probability) * 2</code></pre>
<pre><code>## [1] 0.0713</code></pre>
<p>We can compare our calculation with <code>R</code>, which resulted in:</p>
<pre class="r"><code>fisher.test(survey, alternative=&quot;greater&quot;)$p.value</code></pre>
<pre><code>## [1] 0.0357</code></pre>
<pre class="r"><code>fisher.test(survey, alternative=&quot;two.sided&quot;)$p.value</code></pre>
<pre><code>## [1] 0.0713</code></pre>
</div>
<div id="chi-square-test-of-independence" class="section level1">
<h1>Chi-square Test of Independence</h1>
<p>Previous example demonstrated how Fisher’s test measures exact p-value given a
particular condition. We can imagine, with a larger sample space, the
computation gets more complicated. To provide an approximation, we may use
other measures, such as Chi-square test of independence or G-test. As
Chi-square is more ubiquitous, we will limit our discussion on this approach.
Please be advised though, different method of Chi-square test exists, where we
may choose any based on the assumption on how each variable associates with one
another.</p>
<p>Our demonstration on Fisher’s test depicts how arduous the computation can be.
Pearson’s Chi-square, as one method of approximation, gives a close estimate to
Fisher’s test especially with higher sample size of <span class="math inline">\(n: n \to \infty\)</span>. In
Fisher’s test, we often limit our inference to a <span class="math inline">\(2 \times 2\)</span> contingency
table. Pearson’s Chi-square provides a more generalizable construct where we
can apply <span class="math inline">\(m \times n\)</span> contingency tables into calculation. As its name
suggests, statistics computed using Pearson’s Chi-square follows a Chi-square
distribution, where the degree of freedom <span class="math inline">\(k\)</span> depends on the number of classes
in our variables. Approximating the p-value will requires computing the
Chi-square value, where we define:</p>
<p><span class="math display">\[\begin{align}
\chi^2 &amp;= \displaystyle \sum_{i, j} \frac{(O_{ij} - E_{ij})^2}{E_{ij}} \\
E_{ij} &amp;= \frac{\sum O_i \cdot \sum O_j}{\sum O_i + O_j}
\end{align}\]</span></p>
<p><span class="math inline">\(O:\)</span> Observed outcome<br />
<span class="math inline">\(E:\)</span> Expected outcome<br />
<span class="math inline">\(i, j:\)</span> Elements in the contingency table</p>
<p>Knowing the equation, we may compute the Chi-square value using previously
described contingency table:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Outcome 1
</th>
<th style="text-align:left;">
Outcome 2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Exposure 1
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:left;">
b
</td>
</tr>
<tr>
<td style="text-align:left;">
Exposure 2
</td>
<td style="text-align:left;">
c
</td>
<td style="text-align:left;">
d
</td>
</tr>
</tbody>
</table>
<p>So we will have our expected outcome as:</p>
<p><span class="math display">\[E_{11} = \frac{(a + b) \cdot (a + c)}{a + b + c + d}\]</span> <span class="math display">\[E_{12} = \frac{(a + b) \cdot (b + d)}{a + b + c + d}\]</span></p>
<p><span class="math display">\[E_{21} = \frac{(c + d) \cdot (a + c)}{a + b + c + d}\]</span></p>
<p><span class="math display">\[E_{22} = \frac{(c + d) \cdot (b + d)}{a + b + c + d}\]</span></p>
<p>Considering the value in <span class="math inline">\((a, b, c, d)\)</span>:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Indomaret
</th>
<th style="text-align:right;">
Alfamart
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Suburban
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
20
</td>
</tr>
<tr>
<td style="text-align:left;">
Urban
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
30
</td>
</tr>
</tbody>
</table>
<p>We can calculate the expected outcomes:</p>
<p><span class="math display">\[\begin{align}
E_{ij} &amp;= \frac{\sum O_i \cdot \sum O_j}{\sum O_i + O_j} \\
\\
E_{11} &amp;= 25 \\
E_{12} &amp;= 25 \\
E_{21} &amp;= 25 \\
E_{22} &amp;= 25
\end{align}\]</span></p>
<p>Then we can compute our Chi-square statistics:</p>
<p><span class="math display">\[\begin{align}
\chi^2 &amp;= \displaystyle \sum_{i, j} \frac{(O_{ij} - E_{ij})^2}{E_{ij}} \\
       &amp;= \frac{(30-25)^2}{25} + \frac{(20-25)^2}{25} + \frac{(30-25)^2}{25} + \frac{(20-25)^2}{25} \\
       &amp;= 4
\end{align}\]</span></p>
<p>Great! But it has yet to cough up the p-value. To obtain our p-value, we need
to know where <span class="math inline">\(\chi^2=4\)</span> is located in a Chi-square distribution quantile with
a degree of freedom <span class="math inline">\(k=1\)</span>.</p>
<pre class="r"><code>1 - pchisq(4, df=1)</code></pre>
<pre><code>## [1] 0.0455</code></pre>
<p>How does our finding compare to <code>R</code>?</p>
<pre class="r"><code>chisq.test(survey, correct=FALSE)$p.value</code></pre>
<pre><code>## [1] 0.0455</code></pre>
</div>
<div id="test-of-independence-with-paired-samples" class="section level1">
<h1>Test of Independence with Paired Samples</h1>
<p>When conducting a research, sometimes we are more inclined to see how the same
individual expresses different measurement, either by measuring in a different
time or using a different measurement scale. In other word, we have <em>same</em>
samples and variables, yet <em>different</em> measures, i.e. a <strong>paired sample</strong>. By
mathematical design, Pearson’s Chi-square could not determine differences
happening overtime, as it only depicts proportional differences from a given
contingency table. As a solution, McNemar’s Chi-square provides a more
appropriate estimate of p-value on how the proportion changes by respecting its
temporal order.</p>
<p>We will consider the following scenario: Suppose we continue our market
research, where we ask <strong>exactly same</strong> subjects <em>three months</em> later. We
expected no changes in their preferences of chain-store outlets. It turned out,
regardless of their area of residence, 25 people who previously preferred go to
Indomaret now shop in Alfamart. Meanwhile, 20 people who used to visit Alfamart
now prefer Indomaret.</p>
<p>To capture changes overtime, we can formulate our contingency table as follow:</p>
<table class=" lightable-paper lightable-hover" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Indomaret
</th>
<th style="text-align:right;">
Alfamart
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Indomaret
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
25
</td>
</tr>
<tr>
<td style="text-align:left;">
Alfamart
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
30
</td>
</tr>
</tbody>
</table>
<p>Then, we will set our hypothesis as:</p>
<p><span class="math display">\[\begin{align}
H_0 &amp;: \hat{p_{t_0}} = \hat{p_{t_1}} \\
H_1 &amp;: \hat{p_{t_0}} \neq \hat{p_{t_1}}
\end{align}\]</span></p>
<p>To calculate the Chi-square statistics, McNemar’s proposed following equation:</p>
<p><span class="math display">\[\chi^2 = \frac{(b-c)^2}{b+c}\]</span></p>
<p>In <code>R</code>, we can have the p-value by issuing:</p>
<pre class="r"><code>mcnemar.test(survey2)</code></pre>
<pre><code>## 
##  McNemar&#39;s Chi-squared test with continuity correction
## 
## data:  survey2
## McNemar&#39;s chi-squared = 0.4, df = 1, p-value = 0.6</code></pre>
</div>
<div id="applying-yates-correction" class="section level1">
<h1>Applying Yates’ Correction</h1>
<p>Upon reading the section on exact test and its approximation, we may have
wondered why they provide different results? Is it only depends on the sample
size, or rather, will we have similar results when <span class="math inline">\(n \to \infty\)</span>? The answer
is not quite straightforward, and this section will potentially add more
confusion to the question. When conducting statistical inference, we ought to
satisfy some assumptions on a particular test of our interest. In Fisher’s
test, we need to have a fixed marginal total. Looking at our dummy contingency
table, marginal total is simply the sum of all cells, i.e. <span class="math inline">\(a+b+c+d\)</span>. It is not
a stringent assumption per se, and I personally have not found a strong
evidence on which violation may affect the test robustness. However, as a rule
of thumb based on convenient convention, we only apply Fisher’s test when we
have a low sample count.</p>
<p>Now, we can ask a <em>different</em> question, how low can we consider our data before
applying Fisher’s test? Again, it is a rule of thumb, first we need to have our
contingency table ready for Pearson’s Chi-square test. When we calculate our
expected outcome and have any cell &lt; 5, Fisher’s is a more appropriate test to
conduct. However, in most cases, we may find our data as unsuitable for
Fisher’s exact test, yet we are not sure whether Pearson’s Chi-square will give
a good approximation.</p>
<p>In such a condition, we will have Yates’ correction to give a better estimate.
Yates’ method provides a better estimates and alleviates bias in a <span class="math inline">\(2 \times 2\)</span>
contingency table. With a larger contingency table, we often do not require
Yates’ correction. If we recall Pearson’s Chi-square equation, we can apply
Yates’ correction in following fashion:</p>
<p><span class="math display">\[\chi^2 = \displaystyle \sum_{i, j} \frac{(|O_{ij} - E_{ij}| - 0.5)^2}{E_{ij}} \\\]</span></p>
</div>
<div id="concluding-remakrs" class="section level1">
<h1>Concluding Remakrs</h1>
<ul>
<li>Large sample (&gt; 10 in each cell) <span class="math inline">\(\to\)</span> use approximation</li>
<li>Low sample <span class="math inline">\(\to\)</span> use an exact test</li>
<li><span class="math inline">\(2 \times 2\)</span> contingency with approximation <span class="math inline">\(\to\)</span> apply Yates’ correction</li>
<li>Low sample with <span class="math inline">\(m \times n\)</span> contingency table <span class="math inline">\(\to\)</span> split or do simulation</li>
</ul>
</div>
