---
author: lam
title: "Sample Size and Statistical power"
weight: 4
description: >

  Proving or disproving a research hypothesis requires representative evidence
  which may help us substantiate our claims. In general, we need a large amount
  of data following a rigorous and well-documented procedure. However, inviting
  all potential subjects is not feasible due to time and resource constraint.
  Considering the amount of limitation we have, how do we determine the minimum
  required amount of data to answer our research question?

summary: >

  Proving or disproving a research hypothesis requires representative evidence
  which may help us substantiate our claims. In general, we need a large amount
  of data following a rigorous and well-documented procedure. However, inviting
  all potential subjects is not feasible due to time and resource constraint.
  Considering the amount of limitation we have, how do we determine the minimum
  required amount of data to answer our research question?

date: 2020-10-01
categories: ["statistics", "ukrida"]
tags: ["power", "R", "sampling", "hypothesis"]
slug: 04-power
csl: ../harvard.csl
bibliography: ../ref.bib
draft: false
---



<p><a href="https://lamurian.rbind.io/note/biostat-ukrida/04-power/slide">Slide</a></p>
<p>Proving or disproving a research hypothesis requires representative evidence
which may help us substantiate our claims. In general, we need a large amount
of data following a rigorous and well-documented procedure. However, inviting
all potential subjects is not feasible due to time and resource constraint.
Considering the amount of limitation we have, how do we determine the minimum
required amount of data to answer our research question?</p>
<div id="p-value" class="section level1">
<h1>P-value</h1>
<p>We reject our <span class="math inline">\(H_0\)</span> when our statistical test resulted in a p-value &lt; 0.05, why
do we use 0.05 as our cut-off point? In a simple term, 0.05 simply reflects a
5% chance of having a correct null hypothesis. Though, I am more inclined to
regard it as a probability value. When the probability is small enough, we
reject our <span class="math inline">\(H_0\)</span>. To help us visualize the idea, we can use a formal approach
to answer our hypothesis on (<em>ahem</em>) a simple coin toss.</p>
<pre class="r"><code>set.seed(1)
coin &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), 10, replace=TRUE, prob=rep(1/2, 2)) %T&gt;% print()</code></pre>
<pre><code>##  [1] &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot;</code></pre>
<p>As always, we set <code>H</code> as our outcome of interest. After observing ten coin
tosses, we may find the appeal to formulate our hypotheses, where we may say:</p>
<ul>
<li><span class="math inline">\(H_0: P(X=x) = 0.5\)</span></li>
<li><span class="math inline">\(H_a: P(X=x) \neq 0.5\)</span></li>
</ul>
<p>Assuming independence of a fair coin, it is a Bernoulli trial and follows the
binomial distribution. But, does it? :)</p>
<pre class="r"><code>binom.test(x=sum(coin == &quot;H&quot;), n=length(coin), p=0.5)</code></pre>
<pre><code>## 
##  Exact binomial test
## 
## data:  sum(coin == &quot;H&quot;) and length(coin)
## number of successes = 6, number of trials = 10, p-value = 0.8
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.2624 0.8784
## sample estimates:
## probability of success 
##                    0.6</code></pre>
<p>At this point, we have seen binomial test numerous times, yet we have not
unravel the secret behind this math! Why did we <em>fail to reject</em> the <span class="math inline">\(H_0\)</span>? Or
rather, why does the <em>p-value &gt; 0.05</em>? The question we wish to answer is:</p>
<blockquote>
<p>What is the probability of having 6 <code>H</code> out of 10 Bernoulli trials? Is it &lt;
5%?</p>
</blockquote>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.binom10-1.png" width="90%" /></p>
<p>To answer our question, we may use Binomial probability function to find our
probability. To find a <span class="math inline">\(P(X=6): X \sim B(10, 0.5)\)</span>, <code>R</code> uses following command:</p>
<pre class="r"><code>dbinom(6, 10, 0.5)</code></pre>
<pre><code>## [1] 0.2051</code></pre>
<p>We can manually calculate the p-value as the .amber[sum] of <span class="math inline">\(P(X \geqslant 6)\)</span>.</p>
<pre class="r"><code>2 * (dbinom(6:10, 10, 0.5) %&gt;% sum())</code></pre>
<pre><code>## [1] 0.7539</code></pre>
<p>So we could not distinguish a relative probability of 0.6 and 0.5
from a ten consecutive coin tosses. Interesting. How if we preserve the ratio
of event (3:5) using more trials?</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.binom100-1.png" width="90%" /></p>
<p>As previously done, we can find the probability <span class="math inline">\(P(X=60): X \sim B(100, 0.5)\)</span>
using the Binomial probability function:</p>
<pre class="r"><code>dbinom(60, 100, 0.5)</code></pre>
<pre><code>## [1] 0.01084</code></pre>
<p>And the p-value would be:</p>
<pre class="r"><code>2 * (dbinom(60:100, 100, 0.5) %&gt;% sum())</code></pre>
<pre><code>## [1] 0.05689</code></pre>
<p>We preserved the ratio, why has the probability changed? The reason lies in the
number of trial we conducted. We have observed how the peak and distribution
width changes. With <em>more</em> trial, the narrower observed distributions get.
That, in itself, does not change our critical value at 5%, but they change
where it is located relative to the mid-point. Theoretically, p-value is
<em>difficult</em> to understand. But in practice, it tells you the probability of
having a correct <span class="math inline">\(H_0\)</span>.</p>
<blockquote>
<p>Low p-value <span class="math inline">\(\to\)</span> reject <span class="math inline">\(H_0\)</span></p>
</blockquote>
<p>Cassie Kozyrkov uploaded a splendid example on how to comprehend the p-value:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/9jW9G8MO4PQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</center>
</div>
<div id="significance-level" class="section level1">
<h1>Significance Level</h1>
<p>In interpreting p-value, we use a threshold of 0.05, which reflect our
significance level <span class="math inline">\(\alpha\)</span>. With a higher <span class="math inline">\(\alpha\)</span>, we have more chance to
reject the <span class="math inline">\(H_0\)</span>, and so does our chance get higher when we have more sample.
However, we may need to worry about incorrect rejection, <em>viz.</em> what is the
chance of we incorrectly reject a true <span class="math inline">\(H_0\)</span>?</p>
<p>Suppose we are conducting a study on a potential cancer therapy. We knew giving
the patient a placebo may affect their recovery rate by 50%. We are certain
that giving the new treatment will increase the probability. Tested on 50
patients, 35 showed signs of better quality of life.</p>
<p>Considering I.I.D, we may model our problem as a Binomial distribution <span class="math inline">\(Cured \sim B(50, 0.5)\)</span>, so we can state our hypothesis as:</p>
<p><span class="math display">\[\begin{align}
H_0 &amp;: P(X=35) = 0.5 \\
H_a &amp;: P(X=35) &gt; 0.5
\end{align}\]</span></p>
<p>Since the data follows the Binomial distribution, we can assess our hypothesis
using the Binomial test:</p>
<pre class="r"><code>binom.test(35, 50, 0.5, alternative=&quot;greater&quot;)</code></pre>
<pre><code>## 
##  Exact binomial test
## 
## data:  35 and 50
## number of successes = 35, number of trials = 50, p-value = 0.003
## alternative hypothesis: true probability of success is greater than 0.5
## 95 percent confidence interval:
##  0.5763 1.0000
## sample estimates:
## probability of success 
##                    0.7</code></pre>
<p>With a nice visualization, we can see where our hypothesis located relative to
the midpoint (hint: the blue dot).</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/stat.err.eg2-1.png" width="90%" /></p>
<p>We are assuming <span class="math inline">\(H_a &gt; H_0\)</span>, how do we picture <span class="math inline">\(\alpha\)</span> in our figure? Since
<span class="math inline">\(\alpha\)</span> is the significance level of 0.05, we need to find a point of
<span class="math inline">\(x : P(X=0.05 | 50, 0.5)\)</span>. After doing so, we can find our <em>significance value</em>
in the x-axis (the red dashed line).</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/stat.err.eg3-1.png" width="90%" /></p>
<p>Area to the right of our significance value determines the probability of
getting a type I error <span class="math inline">\(\alpha\)</span>. If we were to make a second distribution
reflecting our <span class="math inline">\(H_a\)</span>, we may observe intersected area reflecting our
probability of getting a type II error <span class="math inline">\(\beta\)</span>. Assuming <span class="math inline">\(H_a\)</span> coming from
similar distribution as <span class="math inline">\(H_0\)</span>, we just need to determine tis parameter.
However, the value of <span class="math inline">\(\beta\)</span> depends on the <span class="math inline">\(H_a\)</span> distribution, and we only
stated <span class="math inline">\(H_a &gt; H_0\)</span>. It means, the parameter in <span class="math inline">\(H_a\)</span> can be any value higher
than 0.5, either be 0.6, 0.7, 0.8, and so on. For our convenience, we will
assign 0.7 as our <span class="math inline">\(H_a\)</span> so we can construct the second binomial distribution.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/stat.err.eg4-1.png" width="90%" /></p>
<p>The area to the left of our significance value in the <em>second</em> <span class="math inline">\(H_a\)</span>
distribution is the probability of getting a type II error <span class="math inline">\(\beta\)</span>. We can
summarize the characteristic of type I and type II statistical error using
following anecdote:</p>
<p><img src="https://mk0codingwithmaxskac.kinstacdn.com/wp-content/uploads/2019/12/type-1-error-type-2-statistical-power-comic.png" width="100%"></p>
<p>Type I statistical error:</p>
<ul>
<li>Incorrectly rejecting the <span class="math inline">\(H_0\)</span></li>
<li>Reflected as <span class="math inline">\(\alpha \to\)</span> shaded area to the right of <span class="math inline">\(H_0\)</span> distribution</li>
<li>A false positive</li>
</ul>
<p>Type II statistical error:</p>
<ul>
<li>Incorrectly accepting the <span class="math inline">\(H_0\)</span></li>
<li>Reflected as <span class="math inline">\(\beta \to\)</span> shaded area to the left of <span class="math inline">\(H_a\)</span> distribution</li>
<li>A false negative</li>
</ul>
</div>
<div id="power-analysis" class="section level1">
<h1>Power Analysis</h1>
<p>So far, we have learnt there ought to be some probability of incorrectly
rejecting the <span class="math inline">\(H_0\)</span>, which highly depends on the significance level <span class="math inline">\(\alpha\)</span>,
rate of type II error <span class="math inline">\(\beta\)</span>, and the sample size. We can adjust <span class="math inline">\(\alpha\)</span> to
reduce the risk of having type I error at the cost of increasing the risk for
type II error. Such a problem is the art of performing statistics, where we may
tend to justify one type of error instead of another. Imagine this case in a
court as a statistical problem. We have charged someone as guilty of robbery
and we need to prove their guilt before sending them to jail. After collecting
some evidence, we may have two possible error:</p>
<ul>
<li>We incorrectly put an innocent person in jail (type I error)</li>
<li>We failed to identify the culprit and let them free (type II error)</li>
</ul>
<p>What is the more appropriate case to justify, letting a criminal free or
arresting an innocent citizen? That is the question we need to deal with in
statistics, of whether we should accept a type I or type II error. However, if
we have more data (analogous to more evidence in court settings), we may reduce
<strong>both</strong> error at the expense of time and effort. A good research demand a
sufficient amount of data to prevent both type of error. Though, please be
advised, we are not getting anywhere near to type III and other special errors
in statistics. For now, understanding the relation between statistical errors
and sample size is enough to introduce us a new topic: statistical power.</p>
<p>In statistics, we define power as a simple equation of <span class="math inline">\(1 - \beta\)</span>. However,
determining the value of <span class="math inline">\(\beta\)</span> is a bit tricky, as we have visually observed.
In essence, a good statistical power helps us to correctly reject the <span class="math inline">\(H_0\)</span>
when it is actually false. As a rule of thumb, we expect to have a statistical
power in the range of 0.8 - 0.9 (higher is better). We can analyze the power
prospectively <em>or</em> retrospectively, depends on the context of what we would
like to prove. When done prospectively, a thorough power analysis can also help
us determining the minimum required sample.</p>
<p>However, conducting a power analysis is not all roses, since it comes with
several caveats to consider. First of all, power analysis depends on formal
methods to use when we design the statistical analysis pipeline. Since it
depends on our statistical methods and study designs, it often does not
generalize that well. It means that, if we were to propose a different
statistical method, we may need to adjust our power analysis. And lastly, power
analysis only tells us the best case scenario <em>estimate</em>.</p>
<p>There are four linked concepts to comprehend the statistical power:</p>
<ul>
<li>Power</li>
<li>Effect size</li>
<li>Sample size</li>
<li>Alpha</li>
</ul>
<p>So far, we have discussed what the other three are, but it is the first time we
hear about effect size. As a disclaimer though, this post only aims to give a
brief overview of what effect size is. We may not delve in further to formally
prove the equation we will later use. In a layman term, effect size measures a
true difference between two hypotheses. Hitherto, numerous conventions exist to
calculate the effect size. The higher the effect size we have reflects in more
statistical power. And as all good thing goes, effect size is one of the most
difficult to obtain.</p>
<p><img src="https://sayingimages.com/wp-content/uploads/one-simply-can-never-have-power-meme.png" width="100%"></p>
<p>To obtain an effect size, we may initially perform a thorough literature
review, conduct a pilot study, <em>or</em> follow Cohen’s recommendation. When
reviewing published articles, we may find some with similar methods as we
proposed. In such cases, we can use their data to estimate the desired effect
size. Meta-analysis technique is sometimes applicable to make a better
estimate. When we fail to get published articles with similar methods and
elaborate results, we may find the appeal of conducting a pilot study. By doing
so, we can get data reflecting our future study. It <strong>is</strong> time consuming, but
it gives us a closer estimate of how our data will be. Also, a pilot study is a
good chance to resolve any unforeseen issue. Following Cohen’s recommendation
may help us determine what effect size we can regard as being adequate in a
prospective power analysis. Still, it depends on what formal test to use, where
we can use a certain threshold to separate a small, medium and large effect
size.</p>
<p>All words and no number would not help us much to understand the context, so we
shall dig into an example! We will use our previous case of a novel cancer
drug trial. Can we recall this figure?</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/stat.err.eg4-1.png" width="90%" /></p>
<p>We can calculate power when we know the probability function <em>and</em> its
parameters, as such:</p>
<p><span class="math display">\[Let\ X \sim B(n, p)\]</span></p>
<p><span class="math display">\[\begin{align}
sig &amp;= x:P(X=1-\alpha\ |\ n, H_0) \\
\beta &amp;= P(X \leqslant sig\ |\ n, H_1) \\
Power &amp;= 1 - \beta
\end{align}\]</span></p>
<p>Plain math does not help us to stick in with the idea, how about some code?</p>
<pre class="r"><code># Set H0, sample size, significance level (alpha)
h0 &lt;- 0.5; size &lt;- 50; alpha.rate &lt;- 0.05

# Find significance value
alpha.value &lt;- qbinom(1 - alpha.rate, size, prob=h0) %T&gt;% print()</code></pre>
<pre><code>## [1] 31</code></pre>
<pre class="r"><code># Determine H1
h1 &lt;- 0.7

# Calculate beta
beta.value &lt;- dbinom(0:alpha.value, size, prob=h1) %&gt;% sum() %T&gt;% print()</code></pre>
<pre><code>## [1] 0.1406</code></pre>
<pre class="r"><code># Calculate power
1 - beta.value</code></pre>
<pre><code>## [1] 0.8594</code></pre>
<p>Now, that is better :) If you find it hard to understand what’s going on, you
may want to focus on commented section (any line started by <code>#</code>), as they
indicates what action we do. Thankfully, we have some ready-to-use packages to
do the computation for us (yay to the devs!), so we <em>do not need</em> to reinvent
the wheel every time.</p>
</div>
<div id="equation-in-calculating-sample-size" class="section level1">
<h1>Equation in Calculating Sample Size</h1>
<p>As in calculating effect sizes, we have numerous equations to find a minimum
sample size. None fits all needs, as it depends on our search context. We will
see popular ones used in general and biomedical science.</p>
<div id="general-equation" class="section level2">
<h2>General Equation</h2>
<p><span class="math display">\[n = \bigg( \frac{Z_{1 - \frac{\alpha}{2}} + Z_{1-\beta}}{ES} \bigg)^2\]</span></p>
<p>In general, the culprit causing us much headaches when calculating the sample
size is effect sizes. The equation itself only consists of plain arithmetical
operations. We shall understand each symbols first before looking the different
effect size measures.</p>
<p><span class="math inline">\(n\)</span>: Number of minimal sample size<br />
<span class="math inline">\(Z_{1 - \frac{\alpha}{2}}\)</span>: Significance value in a standardized normal distribution<br />
<span class="math inline">\(Z_{1-\beta}\)</span>: Power value in a standardized normal distribution<br />
<span class="math inline">\(ES\)</span>: Effect size</p>
<div id="dichotomous-outcome-one-sample" class="section level3">
<h3>Dichotomous outcome, one sample</h3>
<p><span class="math display">\[\begin{align}
H_0 &amp;: p = p_0 \\
ES &amp;= \frac{p_1 - p_0}{\sqrt{p(1-p)}}
\end{align}\]</span></p>
</div>
<div id="dichotomous-outcome-two-independent-samples" class="section level3">
<h3>Dichotomous outcome, two independent samples</h3>
<p><span class="math display">\[\begin{align}
H_0 &amp;: p_1 = p_2 \\
ES &amp;= \frac{|p_1 = p_2|}{\sqrt{p(1-p)}}
\end{align}\]</span></p>
</div>
<div id="continuous-outcome-one-sample" class="section level3">
<h3>Continuous outcome, one sample</h3>
<p><span class="math display">\[\begin{align}
H_0 &amp;: \mu = \mu_0 \\
ES &amp;= \frac{|\mu_1 = \mu_0|}{\sigma}
\end{align}\]</span></p>
</div>
<div id="continuous-outcome-two-independent-samples" class="section level3">
<h3>Continuous outcome, two independent samples</h3>
<p><span class="math display">\[\begin{align}
H_0 &amp;: \mu_1 = \mu_2 \\
ES &amp;= \frac{|\mu_1 = \mu_2|}{\sigma}
\end{align}\]</span></p>
</div>
<div id="continuous-outcome-two-matched-samples" class="section level3">
<h3>Continuous outcome, two matched samples</h3>
<p><span class="math display">\[\begin{align}
H_0 &amp;: \mu_d = 0 \\
ES &amp;= \frac{\mu_d}{\sigma_d}
\end{align}\]</span></p>
<p>Different study designs may require different solution, where different field
of science contributed to countless preferences in performing statistics. What
do we do as biomedical scientist? <span class="citation">(<a href="#ref-Charan2013" role="doc-biblioref">Charan and Biswas, 2013</a>)</span></p>
</div>
</div>
<div id="cross-sectional" class="section level2">
<h2>Cross-Sectional</h2>
<div id="qualitative-variable" class="section level3">
<h3>Qualitative variable</h3>
<p><span class="math display">\[n = \frac{Z_{1-\frac{\alpha}{2}}^2 \cdot p (1-p)}{d^2}\]</span></p>
</div>
</div>
<div id="quantitative-variable" class="section level2">
<h2>Quantitative variable</h2>
<p><span class="math display">\[n = \frac{Z_{1-\frac{\alpha}{2}}^2 \cdot \sigma^2}{d^2}\]</span></p>
<p><span class="math inline">\(Z_{1 - \frac{\alpha}{2}}\)</span>: Significance value in a standardized normal distribution<br />
<span class="math inline">\(d\)</span>: Absolute error as determined by the researcher<br />
<span class="math inline">\(p\)</span>: Estimated proportion<br />
<span class="math inline">\(\sigma\)</span>: Standard deviation</p>
</div>
<div id="case-control" class="section level2">
<h2>Case-Control</h2>
<div id="qualitative-variable-1" class="section level3">
<h3>Qualitative variable</h3>
<p><span class="math display">\[n = \frac{r+1}{r} \frac{(p^*)(1-p^*)(Z_{\beta} + Z_{\frac{\alpha}{2}})^2}{(p_1 - p_2)^2}\]</span></p>
</div>
<div id="quantitative-variable-1" class="section level3">
<h3>Quantitative variable</h3>
<p><span class="math display">\[n = \frac{r+1}{r} \frac{\sigma^2(Z_{\beta} + Z_{\frac{\alpha}{2}})^2}{(p_1 - p_2)^2}\]</span></p>
<p><span class="math inline">\(r\)</span>: Ratio of control to case<br />
<span class="math inline">\(p^*\)</span>: Average of exposed samples proportion<br />
<span class="math inline">\(\sigma\)</span>: Standard deviation from previous publication<br />
<span class="math inline">\(p_1 - p_2\)</span>: Difference in proportion as previously reported<br />
<span class="math inline">\(Z_{\beta}\)</span>: <span class="math inline">\(\beta\)</span> value in a standardized normal distribution</p>
</div>
</div>
<div id="clinical-trial-experimental" class="section level2">
<h2>Clinical Trial / Experimental</h2>
<div id="qualitative-variable-2" class="section level3">
<h3>Qualitative variable</h3>
<p><span class="math display">\[n = \frac{2 P(1-P) \cdot (Z_{\frac{\alpha}{2}} + Z_{\beta})^2}{(p_1-p_2)^2}\]</span></p>
</div>
<div id="quantitative-variable-2" class="section level3">
<h3>Quantitative variable</h3>
<p><span class="math display">\[n = \frac{2\sigma^2 \cdot (Z_{\frac{\alpha}{2}} + Z_{\beta})^2}{d^2}\]</span></p>
<p><span class="math inline">\(\sigma\)</span>: Standard deviation from previous publication<br />
<span class="math inline">\(P\)</span>: Pooled prevalence from both groups<br />
<span class="math inline">\(p_1 - p_2\)</span>: Difference in proportion as previously reported</p>
</div>
</div>
</div>
<div id="random-sampling" class="section level1">
<h1>Random Sampling</h1>
<p>After understanding what p-value is and calculating the minimum sample size, we
are one step closer to conducting our own investigation. We have previously
discussed the importance of having independent and identically distributed
data, where random sampling is one method to fulfill such an assumption. In
random sampling, we can choose either a non-probability or a probability method
to acquire our sample. In following sections, we will have a brief explanation
on each of available methods.</p>
<div id="non-probability-random-sampling" class="section level2">
<h2>Non-Probability Random Sampling</h2>
<div id="convenience" class="section level3">
<h3>Convenience</h3>
<ul>
<li>Based on availability</li>
<li>Representativeness is unknown</li>
<li>Useful in preliminary study</li>
</ul>
</div>
<div id="quota" class="section level3">
<h3>Quota</h3>
<ul>
<li>As in convenient sampling</li>
<li>We set the desired proportion of our sample</li>
<li>Proportion based on specific criteria, e.g. age, sex, etc.</li>
</ul>
</div>
</div>
<div id="probability-random-sampling" class="section level2">
<h2>Probability Random Sampling</h2>
<div id="simple" class="section level3">
<h3>Simple</h3>
<ul>
<li>Random sample from a list of all subjects in a population</li>
<li>Each subject has an equal chance to participate</li>
<li>Useful in a small population</li>
</ul>
</div>
<div id="systematic" class="section level3">
<h3>Systematic</h3>
<ul>
<li>Subject selection not entirely random</li>
<li>As in random sampling, requires an enumeration of all subjects</li>
<li>Systematically select the subject based on a certain criteria, e.g. every <span class="math inline">\(n_{th}\)</span> subject</li>
</ul>
</div>
<div id="stratified-cluster" class="section level3">
<h3>Stratified / cluster</h3>
<ul>
<li>Split subjects into stratified / clustered groups</li>
<li>Do random sampling from each group</li>
<li>Stratified <span class="math inline">\(\to\)</span> preserves ordinality, i.e. the order is important</li>
</ul>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Charan2013" class="csl-entry">
Charan, J. and Biswas, T. (2013) How to calculate sample size for different study designs in medical research? <em>Indian Journal of Psychological Medicine</em> [online]. 35 (2), pp. 121. Available from: <a href="https://doi.org/10.4103/0253-7176.116232">https://doi.org/10.4103/0253-7176.116232</a>doi:<a href="https://doi.org/10.4103/0253-7176.116232">10.4103/0253-7176.116232</a>.
</div>
</div>
</div>
</div>
</div>
