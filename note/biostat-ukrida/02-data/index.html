---
author: lam
title: "Data: Type and Distribution"
weight: 2
description: >
  To comprehend our surroundings, we rely on processed sensory inputs and infer
  the best action to respond with. In a more complex (or complicated)
  situation, we often find recorded inputs more reliable than our sole
  perception. Recorded inputs are data, which may or may not follow a certain
  structure. In essence, data is distinguishable based on their properties,
  resulting in different data types. With more collected data, a particular
  pattern emerges, where we can observe a peculiar behaviour from one pattern
  compared to the other. Here we discuss the nature of data, viz. its type
  and distribution.
summary: >
  To comprehend our surroundings, we rely on processed sensory inputs and infer
  the best action to respond with. In a more complex (or complicated)
  situation, we often find recorded inputs more reliable than our sole
  perception. Recorded inputs are data, which may or may not follow a certain
  structure. In essence, data is distinguishable based on their properties,
  resulting in different data types. With more collected data, a particular
  pattern emerges, where we can observe a peculiar behaviour from one pattern
  compared to the other. Here we discuss the nature of data, viz. its type
  and distribution.
date: 2020-09-15
slides: https://lamurian.rbind.io/note/biostat-ukrida/01-intro/slide
categories: ["statistics", "ukrida"]
tags: ["descriptive statistics", "R", "probability theory"]
slug: 02-data
csl: ../harvard.csl
bibliography: ../ref.bib
draft: false
---



<p><a href="https://lamurian.rbind.io/note/biostat-ukrida/02-data/slide">Slide</a></p>
<p>To comprehend our surroundings, we rely on processed sensory inputs and infer
the best action to respond with. In a more complex (or complicated) situation, we
often find recorded inputs more reliable than our sole perception. Recorded
inputs are data, which may or may not follow a certain structure. In essence,
data is distinguishable based on their properties, resulting in different data
types. With more collected data, a particular pattern emerges, where we can
observe a peculiar behaviour from one pattern compared to the other. Here we
discuss the nature of data, <em>viz.</em> its type and distribution.</p>
<div id="type-of-data" class="section level1">
<h1>Type of Data</h1>
<p>Exist are numerous conventions in describing data. However, we shall keep in
mind that understanding the nature behind categorical and numeric data is more
important. We may further divide categorical data into nominal and ordinal,
while numeric into discrete and continuous. Looking at continuous data, it is
also of interest to distinguish interval and ratio data.</p>
<div id="categorical---nominal" class="section level2">
<h2>Categorical - Nominal</h2>
<p>Nominal data are observable through its distinctive characteristics.
Differences seen in nominal data is not directly comparable, where it subsists
only to help identifying one object from another. The example of nominal data
includes genders, brand of a certain product, animal species, type of tea, and
the list goes further. It is important to notice that nominal data does not
only come in two classes, but also multiple classes.</p>
<p><img src="https://www.incimages.com/uploaded_files/image/970x450/male-female-sign-1940x900_35330.jpg" width="100%"></p>
</div>
<div id="categorical---ordinal" class="section level2">
<h2>Categorical - Ordinal</h2>
<p>The defining attribute of ordinal data is existing stratification among its
member, enable a direct comparison. There are numerous examples on ordinal
data, where spiciness level, education background, malignancy and severity are
to name but a few. As we shall imagine, we can directly compare spicy foods to
less spicy ones. We could not confidently say the same with nominal data, as we
cannot determine which brand is more than another. For instances, we cannot say
Anchor as being more butter compared to Wijsman, that does not make any sense.
But we can say cayenne peppers as being spicier compared to bird’s eye chili
peppers.</p>
<p><img src="https://static.vecteezy.com/system/resources/previews/000/680/216/original/spicy-level-of-red-hot-pepper.jpg" width="100%"></p>
</div>
<div id="numeric---discrete" class="section level2">
<h2>Numeric - Discrete</h2>
<p>A discrete data is countable, indicating its availability to a subset of
arithmetical operations, including addition and subtraction. Any frequency
measure is a type of discrete numeric data. Previously, we have seen the
difference between ordinal and nominal data. However, any class from ordinal or
nominal is a subject to frequency measure. It means that we can count the
number of female students in a university, also the how many bird’s eye chilli
peppers we have in a box. In such a situation, both female student and a
particular type of chilli pepper still belongs categorical data, respectively.
But we shall see that the count is a discrete numeric data. So, in essence,
discrete data is the result of frequency counting. In other words,
<strong>countable</strong> defines the discrete numeric data. Other examples of discrete
data includes the number of apples in a store, jugs arranged in a shelf,
plastic bottle wastes on the shore, and so on.</p>
<p><img src="https://nypost.com/wp-content/uploads/sites/2/2018/03/180315-water-bottles-feature-image.jpg?quality=90&strip=all&w=1200" width="100%"></p>
</div>
<div id="numeric---continuous" class="section level2">
<h2>Numeric - Continuous</h2>
<p>If being countable belongs to discrete data, then <strong>measurable</strong> is the main
characteristics of continuous numeric data. Continuous data assumes the
availability of its subjects to a certain measurement instrument. In measuring
an object, we may stumble upon a problem of whether we are looking at its
interval or the actual ratio. What is the difference?</p>
<p><img src="https://livelaughloveandlose.files.wordpress.com/2016/05/weighing-scales.jpg" width="100%"></p>
<div id="interval" class="section level3">
<h3>Interval</h3>
<p>Interval data has no absolute zero, but it presents with a <strong>fixed distance</strong>.
Some arithmetical operations are applicable, including addition and
subtraction. Examples of this data type includes temperature in scales other
than Kelvin. To make sense interval data, imagine it this way: you are
measuring water temperature using a thermometer. Suppose you have two cups of
water, one filled with hot and another with cold water. You measured both cups,
where the hot one is about 40<sup>o</sup>C, and the cold one is 20<sup>o</sup>C. We can say the
hot cup as being warmer, but we cannot state 40<sup>o</sup>C being twice as hot as
20<sup>o</sup>C because Celcius is a relative measure. We shall see how Kelvin differ in
that respect.</p>
</div>
<div id="ratio" class="section level3">
<h3>Ratio</h3>
<p>In ratio data, we do not assume a fixed distance, as the measure may take an
infinitesimal values. However, ratio has an absolute zero. Meaning that any
measurement of <span class="math inline">\(x\)</span> has elements of positive real number <span class="math inline">\(\mathbb{R}\)</span>, with zero being
the lower bound, or simply <span class="math inline">\(x \in \mathbb{R}: x \geqslant 0\)</span>. Examples on ratio
data includes weight, volume and temperature in Kelvin. Kelvin approximates an
entropy within a system, meaning that it standardizes the amount of heat
dissipated or taken by a particular object. So in case we have two cups with
different temperature measured in Kelvin, we can argue one being a few times
hotter than the other.</p>
</div>
</div>
</div>
<div id="a-glimpse-into-probability" class="section level1">
<h1>A Glimpse Into Probability</h1>
<p>Being able to collect data means we can process them to produce an inference of
our surrounding. I may overuse (or even tend to abuse) the word “inference” in
this post, but please bear with it. We want to make sense of our world using
the data we have. For example, suppose we are ordering food which came with
cayenne chilli peppers. We knew from previous experience that out of ten times
consuming cayenne peppers, three resulted in having a diarrhoea. Now, we would
like to know the possibility of having diarrhoea if we proceed to eat this chilli
pepper. In essence, we are facing a probability problem, i.e. what is the
chance of having a diarrhoea after consuming cayenne chilli pepper?</p>
<p>In less sophisticated manner, we may regard probability as a relative
frequency. It means that we are estimating the parameter proportion using
statistical approach. If out of ten times we got three occurrences, we are
more inclined to say that our chance is roughly 30%. In measuring probability,
we need to know the event and its sample space. An event <span class="math inline">\(E\)</span> is an expected result,
whereas the sample space <span class="math inline">\(S\)</span> is all possible outcomes. Therefore, we can
calculate the probability of a particular event as follow:</p>
<p><span class="math display">\[P(E=e) = \frac{E}{S}\]</span></p>
<p>Now suppose we have a fair coin and we intend to flip it 10 times, where <code>H</code>
indicates the head and <code>T</code> the tail. In such a case, our sample space is:</p>
<pre class="r"><code>set.seed(1)
S &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), 10, replace=TRUE, prob=rep(1/2, 2)) %T&gt;% print()</code></pre>
<pre><code>##  [1] &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot;</code></pre>
<p>Let the head be our expected outcomes, so we can list our event as:</p>
<pre class="r"><code>E &lt;- S[which(S == &quot;H&quot;)] %T&gt;% print()</code></pre>
<pre><code>## [1] &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot;</code></pre>
<p>Thus, we can regard the probability of having a desired outcome as a relative
frequency of events in a given sample space:</p>
<pre class="r"><code>length(E) / length(S)</code></pre>
<pre><code>## [1] 0.6</code></pre>
<p>So, ten flips using a fair coin resulted in a
60% chance of having heads. The method we use so
far to determine the probability is an enumeration, i.e. listing all desired
events and dividing it to the sample space. However, higher sample space makes
it harder to solve, and it is more apparent with a sequential problem. In a
sequential problem, we have to calculate the probability from multiple
instances, e.g.: having two heads consecutively.</p>
<p>Other methods in estimating the probability is the tree diagram and resampling
method. The tree diagram is available to solve a more complex probability
problem. Suppose we have an urn evenly filled with identical balls, which only
difference is their colours. Let the color be red and blue, which our urn
contains 50 and 30 balls of each red and blue, respectively. If we take three
balls without replacement, how high is the chance of getting three blue balls?
This is the urn problem, a classic from statistics and probability theory. If
we were to build a tree diagram, it will look as follow (<code>B</code> stands for blue
and <code>R</code> for red):</p>
<pre><code>                        / B (28/78)
               B (29/79)
             /          \ R (50/78)
    B (30/80)
   /         \
  /            R (50/79)
80
  \
   \          
    R (50/80)</code></pre>
<p>Please note it is an incomplete tree diagram. Of course, you may fill out the
rest, but we are only interested with the probability of having three blue
balls. If we take only the upper branches, representing three consecutive draws
of the blue ball, we get the probability of
0.0494. For this type of problem, tree diagram is
practically easier than listing all probable outcomes and desired events.</p>
<p>To understand the resampling method, we will conduct a quick experiment, where
we roll a dice numerous times. Since we do not physically own a dice –even
if we do, I would not suggest doing this experiment physically– we will make a
simple function to do the dice roll for us. I will call this function <code>dice</code>.
Very creative, huh? ;)</p>
<pre class="r"><code>dice &lt;- function(n) {
    sample(1:6, n, replace=TRUE, prob=rep(1/6, 6))
}</code></pre>
<p>We want to know whether our function work:</p>
<pre class="r"><code>dice(1)</code></pre>
<pre><code>## [1] 3</code></pre>
<p>It does! Now, we can safely proceed and roll the dice ten times. Let 4 be our
outcome of interest. How high is the probability of having the event within 10
trials?</p>
<pre class="r"><code>set.seed(1)
roll &lt;- dice(10) %T&gt;% print()</code></pre>
<pre><code>##  [1] 3 4 5 1 3 1 1 5 5 2</code></pre>
<p>Turns out, the probability of getting 4 is
1/10. As we have a fair dice, why is
the probability not 1/6? The clue is a sample and the population. We only have
a subset of its population. Ideally, we will need to roll the dice <span class="math inline">\(n\)</span> number
of time, with <span class="math inline">\(n \to \infty\)</span>. The more sample we have, the closer it is to
represent the population. What will we get with different number of rolls?</p>
<pre class="r"><code>roll &lt;- c(10, 100, 1000, 10000, 100000, 1000000, 10000000)
prob &lt;- sapply(roll, function(n) {
    set.seed(1); roll &lt;- dice(n)
    sum(roll==4) / length(roll)
})</code></pre>
<p>Theoretically, the error of estimated probability is inversely proportional to
the number of trial, where we can compute as:</p>
<p><span class="math display">\[\epsilon = \sqrt{\frac{\hat{p} (1-\hat{p})}{N}},\ where:\]</span></p>
<p><span class="math inline">\(\epsilon\)</span>: Error<br />
<span class="math inline">\(\hat{p}\)</span>: Estimated probability (current trial)<br />
<span class="math inline">\(N\)</span>: Number of resampling</p>
<p>To compute the error in our experiment, we will make a function as follow:</p>
<pre class="r"><code>epsilon &lt;- function(p.hat, n) {
    sqrt({p.hat * (1-p.hat)}/n)
}</code></pre>
<pre class="r"><code>df &lt;- data.frame(list(&quot;roll&quot;=roll, &quot;prob&quot;=prob))</code></pre>
<pre class="r"><code>df$error &lt;- mapply(function(p.hat, n) {
    epsilon(p.hat, n)
}, p.hat=df$prob, n=df$roll) %T&gt;% print()</code></pre>
<pre><code>## [1] 0.0948683 0.0433013 0.0126491 0.0037773 0.0011769 0.0003724 0.0001178</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.trial-1.png" width="100%" /></p>
<p>With more trials, we get closer to the expected probability in a fair dice,
which is 1/6, or equivalently 0.1667.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.error-1.png" width="100%" /></p>
<p>Following previous theory, the error is decreasing proportionally to the number
of sample.</p>
</div>
<div id="random-variables" class="section level1">
<h1>Random Variables</h1>
<p>We often heard, or at least will often hear, about independent and identically
distributed (I.I.D) random variables. I.I.D simply states all sampled random
variables should be independent from one another. Each sampling instance should
also follow an identical procedure. With I.I.D, we can do a better probability
approximation using:</p>
<ul>
<li>Probability Mass Function (PMF) in discrete variable</li>
<li>Probability Density Function (PDF) in continuous variable</li>
</ul>
<p>The function explained in this section is arbitrary, where we will
take a deeper look on most used ones. Mathematically, we can have any
probability function as long as it satisfies following rules:</p>
<p><span class="math display">\[\begin{align}
P(E=e) &amp;= f(e) &gt; 0: E \in S \tag{1} \\
\displaystyle \sum_{e \in S} f(e) &amp;= 1 \tag{2} \\
P(E \in A) &amp;= \displaystyle \sum_{e \in A} f(x) \tag{3}: A \subset S
\end{align}\]</span></p>
</div>
<div id="discrete-distribution" class="section level1">
<h1>Discrete Distribution</h1>
<p>In previous subsection explaining probability theory, we have done coin
flipping to see how probability works. Coin flipping is a situation where we
expect two possible outcomes, in which we desire a certain event to happen. In
essence, we have conducted a Bernoulli trial, which is:</p>
<ul>
<li>An experiment with only two outcomes</li>
<li>All trials being independent from each other</li>
<li>All instances have an identical probability</li>
</ul>
<p>We shall see some probability functions describing series of Bernoulli event.</p>
<div id="binomial-distribution" class="section level2">
<h2>Binomial Distribution</h2>
<p>Binomial is a type of discrete distribution with an identical iteration over
<span class="math inline">\(n\)</span> times of trial. Each iteration is an independent instances corresponding to
a Bernoulli trial. Binomial distribution uses the following PMF, which explain
the probability of having desired outcome (an event) within a certain amount of
time given <span class="math inline">\(n\)</span> times of trial.</p>
<p><span class="math display">\[\begin{align}
f(x) &amp;= \binom{n}{x} p^x (1-p)^{n-x} \tag{1} \\
\binom{n}{x} &amp;= \frac{n!}{x! (n-x)!}
\end{align}\]</span></p>
<p>Or simply denoted as: <span class="math inline">\(X \sim B(n, p)\)</span></p>
<p>Knowing the PMF and its parameters, we can calculate the descriptive
statistics. From this point onward, this post will only consider explaining
mean and standard deviation. We will find their use in comprehending the
Central Limit Theorem (CLT).</p>
<p><span class="math display">\[\begin{align}
\mu &amp;= n \cdot p \\
\sigma &amp;= \sqrt{\mu \cdot (1-p)}
\end{align}\]</span></p>
<p>By changing the parameters, we can see the behaviour of binomial distribution
as depicted by following figures:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.binom.n-1.png" width="100%" /></p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.binom.p-1.png" width="100%" /></p>
</div>
<div id="geometric-distribution" class="section level2">
<h2>Geometric Distribution</h2>
<p>Suppose we are conducting Bernoulli trials, then we are interested to see the
number of trial to conduct before getting an event. In other words, we are
looking for a way to calculate the probability of having a particular event
after <span class="math inline">\(k\)</span>-th number of failures. Geometric distribution is a derivation of
binomial distribution, where we use <span class="math inline">\(x=1\)</span> in <span class="math inline">\(n\)</span> number of trials. So we can
formulate geometric distribution as:</p>
<p><span class="math display">\[f(n) = P(X=n) = p (1-p)^{n-1}, with:\]</span></p>
<p><span class="math inline">\(n\)</span>: Number of trials to get an event<br />
<span class="math inline">\(p\)</span>: The probability of getting an event<br />
Or simply denoted as <span class="math inline">\(X \sim G(p)\)</span></p>
<p><span class="math display">\[\begin{align}
\mu &amp;= \frac{1}{p} \\
\sigma &amp;= \sqrt{\frac{1-p}{p^2}}
\end{align}\]</span></p>
<p>Using different value of <span class="math inline">\(p\)</span> as its parameter, we can obtain following
probability distribution:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.geom-1.png" width="100%" /></p>
</div>
<div id="poisson-distribution" class="section level2">
<h2>Poisson Distribution</h2>
<p>Suppose we know the rate of certain outcomes, then Poisson distribution may
define the probability of an outcome happening <span class="math inline">\(x\)</span> times. The time unit in
Poisson distribution is discrete, as we only limit our observation within a
particular time frame (observation period). Poisson distribution uses following
PMF:</p>
<p><span class="math display">\[f(x) = \frac{e^{-\lambda}\lambda^x}{x!},\ with:\]</span></p>
<p><span class="math inline">\(x\)</span>: The number of expected events<br />
<span class="math inline">\(e\)</span>: Euler’s number<br />
<span class="math inline">\(\lambda\)</span>: Average number of events in one time frame</p>
<p>Or simply denoted as <span class="math inline">\(X \sim P(\lambda)\)</span></p>
<p><span class="math display">\[\begin{align}
\mu &amp;= \lambda \\
\sigma &amp;= \sqrt{\lambda}
\end{align}\]</span></p>
<p>Altering the <span class="math inline">\(\lambda\)</span> in its parameter, we can see how Poisson distribution
behave:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.pois-1.png" width="100%" /></p>
</div>
</div>
<div id="continuous-distribution" class="section level1">
<h1>Continuous Distribution</h1>
<p>So far, we have seen three commonly used distributions in explaining discrete
variables, which often came from a Bernoulli trial. Understanding the essence
behind probability distribution has tempted us to find a plausible explanation
on continuous distributions. However, since we consider continuity, it makes no
sense to use PMF, because PMF defines a probability in a given discrete
measure. We will look into a probability density function (PDF) and use
histogram with density plot to depict probabilities in continuous measure.</p>
<div id="uniform-distribution" class="section level2">
<h2>Uniform Distribution</h2>
<p>As its name suggests, the uniform distribution is a type of continuous
distribution which describe (drum roll intensifies) uniform probabilities. We
may not find its potential uses in common statistical procedure, but the
uniform distribution is a capable tool to generate random numbers. This is
useful when you need a properly randomized situation. For example, when you
randomly assign patient groups in a clinical trials.</p>
<p><span class="math display">\[f(x) = \frac{1}{b-a}\]</span></p>
<p>Or simply denoted as <span class="math inline">\(X \sim U(a,b)\)</span></p>
<p><span class="math display">\[\begin{align}
\mu &amp;= \frac{b+a}{2} \\
\sigma &amp;= \frac{(b-a)^2}{12}
\end{align}\]</span></p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.unif-1.png" width="100%" /></p>
</div>
<div id="exponential-distribution" class="section level2">
<h2>Exponential Distribution</h2>
<p>Previously, we used Poisson distribution to find the probability of having <span class="math inline">\(n\)</span>
number of events given a particular occurrence rate. If we are interested in
finding the minimum required time to observe an event, we shall re-parameterize
Poisson distribution. Exponential PDF calculate the probability of spending a
certain amount of time to observe an event in a Poisson process. Please be
advised though, as it is not always about the time. Time frame is an intangible
definition, as it could be other continuous measures in a Poisson process such
as mileage, weight, volume, etc.</p>
<p><span class="math display">\[f(x) = \lambda e^{-x \lambda}, with:\]</span></p>
<p><span class="math inline">\(x\)</span>: Time needed to observe an event<br />
<span class="math inline">\(\lambda\)</span>: The rate for a certain event</p>
<p>Or simply denoted as <span class="math inline">\(X \sim Exponential(\lambda)\)</span></p>
<p><span class="math display">\[\mu = \sigma = \frac{1}{\lambda}\]</span></p>
<p>Changing the <span class="math inline">\(\lambda\)</span> parameters may result in following figure:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.exp-1.png" width="100%" /></p>
</div>
<div id="gamma-distribution" class="section level2">
<h2>Gamma Distribution</h2>
<p>Exponential distribution is a good approximate to surmise the probability on
spending a particular amount of time for <strong>one</strong> event to occur. However, when
we are interested to find the probability over <strong>numerous</strong> amount of events,
then Gamma PDF can model a more suitable distribution. Gamma PDF only differs
to the exponential PDF with respect to its shape parameter. When we set the
shape <span class="math inline">\(\alpha=1\)</span> in Gamma PDF, we can get the exponential PDF.</p>
<p><span class="math display">\[\begin{align}
f(x) &amp;= \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-x \beta} \\
\Gamma(\alpha) &amp;= \displaystyle \int_0^\infty y^{\alpha -1} e^{-y}\ dy,\ with:
\end{align}\]</span></p>
<p><span class="math inline">\(\beta\)</span>: Rate ( <span class="math inline">\(\lambda\)</span> in exponential PDF)<br />
<span class="math inline">\(\alpha\)</span>: Shape<br />
<span class="math inline">\(\Gamma\)</span>: Gamma function<br />
<span class="math inline">\(e\)</span>: Euler number</p>
<p>Or simply denoted as <span class="math inline">\(X \sim \Gamma(\alpha, \beta)\)</span></p>
<p><span class="math display">\[\begin{align}
\mu &amp;= \frac{\alpha}{\beta} \\
\sigma &amp;= \frac{\sqrt{\alpha}}{\beta}
\end{align}\]</span></p>
<p>In exponential distribution, we observed how the distribution changes over
different <span class="math inline">\(\lambda\)</span> values. Since the effect of rate <span class="math inline">\(\beta\)</span> parameter is
somewhat similar in Gamma PDF, we will see how the distribution looks over
different values of shape <span class="math inline">\(\alpha\)</span> parameter.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.gamma-1.png" width="100%" /></p>
</div>
<div id="chi2-distributions" class="section level2">
<h2><span class="math inline">\(\chi^2\)</span> Distributions</h2>
<p>As special cases of Gamma distribution, <span class="math inline">\(\chi^2\)</span> PDF defines a family of
distributions widely used in statistical inferences. This post will not go in a
full length to describe the <span class="math inline">\(\chi^2\)</span> distributions. Instead, we shall take a
look at the general form and later see its application on upcoming lectures.</p>
<p><span class="math display">\[f(x) = \frac{1}{\Gamma (k/2) 2^{k/2}} x^{k/2 - 1} e^{-x/2},\ with:\]</span></p>
<p><span class="math inline">\(k\)</span>: Degree of freedom<br />
The rest are Gamma PDF derivations</p>
<p>Or simply denoted as <span class="math inline">\(X \sim \chi^2(k)\)</span></p>
<p><span class="math display">\[\begin{align}
\mu &amp;= k \\
\sigma &amp;= \sqrt{2k}
\end{align}\]</span></p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.chisq-1.png" width="100%" /></p>
</div>
<div id="normal-distribution" class="section level2">
<h2>Normal Distribution</h2>
<p>Normal PDF is arguably the most ubiquitous probability function in modelling real-world
data. It is a symmetric distribution where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> completely
describe the probability.</p>
<p><span class="math display">\[f(x) = \frac{1}{\sigma \sqrt{2\pi}}exp \bigg\{ -\frac12 \bigg( \frac{x-\mu}{\sigma} \bigg)^2 \bigg\},\ with:\]</span></p>
<p><span class="math inline">\(x \in \mathbb{R}: -\infty &lt; x &lt; \infty\)</span><br />
<span class="math inline">\(\mu \in \mathbb{R}: -\infty &lt; \mu &lt; \infty\)</span><br />
<span class="math inline">\(\sigma \in \mathbb{R}: 0 &lt; \sigma &lt; \infty\)</span></p>
<p>Or simply denoted as <span class="math inline">\(X \sim N(\mu, \sigma)\)</span></p>
<p>By alternating the parameter <span class="math inline">\(\mu\)</span>, we may observe following instances:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.norm.mu-1.png" width="100%" /></p>
<p>While changing the parameter <span class="math inline">\(\sigma\)</span> will result as follow:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.norm.sigma-1.png" width="100%" /></p>
</div>
</div>
<div id="goodness-of-fit-test" class="section level1">
<h1>Goodness of Fit Test</h1>
<p>To determine whether our data follow a particular distribution, we ought to
conduct a statistical analysis called goodness of fit test. Numerous methods
exist, but we will dig into more popular ones. Given correct parameters, some
methods can fully describe the data. Since we consider doing a statistical
analysis, it is always a good practice to clearly state the hypothesis:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: Given data follow a certain distribution</li>
<li><span class="math inline">\(H_1\)</span>: Given data does not follow a certain distribution</li>
</ul>
<div id="binomial-test" class="section level2">
<h2>Binomial Test</h2>
<p>As a goodness of fit measure, binomial test is an adaptation of binomial PMF.
It determines whether acquired relative frequency is following after Bernoulli trial’s
probability. First we need to compute the probability using given parameters:</p>
<p><span class="math display">\[Pr(X=k) = \binom{n}{k}p^k (1-p)^{n-k}\]</span></p>
<p>Remember we previously tossed a coin 10 times? We will use that example to
perform a binomial test.</p>
<pre class="r"><code>set.seed(1)
S &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), 10, replace=TRUE, prob=rep(1/2, 2)) %T&gt;% print()</code></pre>
<pre><code>##  [1] &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot;</code></pre>
<pre class="r"><code>length(E) / length(S)</code></pre>
<pre><code>## [1] 0.6</code></pre>
<p>If it corresponds with a Bernoulli trial, it should satisfy <span class="math inline">\(P(X=6)\)</span> in such a way
that we cannot reject the <span class="math inline">\(H_0\)</span> when calculating its probability:</p>
<p><span class="math display">\[P(X=6) = \binom{10}{6}0.5^6(1-0.5)^4\]</span></p>
<p>Luckily, we do not need to compute it by hand (yet). We can use following
function in <code>R</code>.</p>
<pre class="r"><code>binom.test(x=6, n=10, p=0.5)</code></pre>
<pre><code>## 
##  Exact binomial test
## 
## data:  6 and 10
## number of successes = 6, number of trials = 10, p-value = 0.8
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.2624 0.8784
## sample estimates:
## probability of success 
##                    0.6</code></pre>
<p>Interpreting the p-value, we cannot reject the <span class="math inline">\(H_0\)</span>, so our coin toss followed
a Bernoulli trial’s probability after all.</p>
</div>
<div id="kolmogorov-smirnov-test" class="section level2">
<h2>Kolmogorov-Smirnov Test</h2>
<p>Often referred as KS test, it is available to determine various distributions
using a non-parametric method. Power-wise, it is pretty much robust and
comparable to the Anderson-Darling test and Shapiro-Wilk on normal
distribution. In following example, we will demonstrate how to conduct a KS
test over an exponential distribution.</p>
<p>Let <span class="math inline">\(X \sim Exponential(2): n = 100\)</span></p>
<pre class="r"><code>set.seed(1)
X &lt;- rexp(n=100, rate=2)</code></pre>
<p>By imputing <span class="math inline">\(\lambda\)</span> variable, Kolmogorov-Smirnov can compute its goodness of fit</p>
<pre class="r"><code>ks.test(X, pexp, rate=2)</code></pre>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  X
## D = 0.084, p-value = 0.5
## alternative hypothesis: two-sided</code></pre>
<p>With p-value &gt; 0.05, we can confidently keep the <span class="math inline">\(H_0\)</span>, that our data follow a
certain distribution. In this case, <span class="math inline">\(X \sim Exponential(2)\)</span>.</p>
</div>
<div id="visual-examination" class="section level2">
<h2>Visual Examination</h2>
<p>Conducting a numerical analysis gives us various advantage when reporting our
results. However, in a large sample, even a small deviation will result in
<span class="math inline">\(H_0\)</span> rejection. Which mean, previously mentioned tests are of no use! Still,
we can rely on some visual cues to determine the distribution. For this
demonstration we will again use the previous object of <span class="math inline">\(X\)</span>.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.gof.hist-1.png" width="100%" /></p>
<p>That is a good start! Sadly, this does not give a clear indication on which
distribution it follows.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.gof.qq-1.png" width="100%" /></p>
<p>Quantile-Quantile Plot (QQ Plot) can give a better visual cue :)</p>
</div>
</div>
<div id="test-of-normality" class="section level1">
<h1>Test of Normality</h1>
<p>Normality test is practically a subset of goodness of fit test, as it only
describes the normal distribution. Some tests are more appropriate under
certain circumstances. Following sections will look at widely used normality
tests in statistics. As always, we will begin by stating the hypothesis:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: Sample follows the normal distribution</li>
<li><span class="math inline">\(H_0\)</span>: Sample does not follow the normal distribution</li>
</ul>
<p>For demonstration purpose, we will populate normally-distributed data into an
object called <span class="math inline">\(X\)</span>, where we let <span class="math inline">\(X \sim N(0, 1): n=100\)</span>.</p>
<pre class="r"><code>set.seed(1)
X &lt;- rnorm(n=100, mean=0, sd=1)</code></pre>
<div id="shapiro-wilk-test" class="section level2">
<h2>Shapiro-Wilk Test</h2>
<p>Shapiro-Wilk is a well-established normality test to assess (<em>ahem</em>) normality.
It can tolerate skewness to a certain degree. Its implementation in <code>R</code> has a
constraint, where the sample size of <span class="math inline">\(n\)</span> should be <span class="math inline">\(3 \leqslant n \leqslant 5000\)</span>.</p>
<pre class="r"><code>shapiro.test(X)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  X
## W = 1, p-value = 1</code></pre>
</div>
<div id="anderson-darling-test" class="section level2">
<h2>Anderson-Darling Test</h2>
<p>Compared to Shapiro-Wilk and Kolmogorov-Smirnov, Anderson-Darling test is less
well known. However, we ought to know this test because Anderson-Darling gives
more weight to the tails when testing for normality. It means, some data with
higher frequencies on its tails, may receive better justification using
Anderson-Darling test. Anderson-Darling implementation in <code>R</code> requires the
package <code>nortest</code>, and it needs at minimum a sample size <span class="math inline">\(n\)</span> of 7.</p>
<pre class="r"><code>nortest::ad.test(X)</code></pre>
<pre><code>## 
##  Anderson-Darling normality test
## 
## data:  X
## A = 0.16, p-value = 0.9</code></pre>
</div>
<div id="visual-examination-1" class="section level2">
<h2>Visual Examination</h2>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/nortest.demo4-1.png" width="100%" /></p>
</div>
<div id="chi2-and-normal-distribution" class="section level2">
<h2><span class="math inline">\(\chi^2\)</span> and Normal Distribution</h2>
<p>We previously mentioned <span class="math inline">\(\chi^2\)</span> distributions as special cases of Gamma
distribution. It turned out, if we raise a normally-distributed data to the
power of 2, it behaves as a <span class="math inline">\(\chi^2\)</span> distribution with 1 degree of freedom.
The following example on <span class="math inline">\(X \sim N(0,1): n=100\)</span> will demonstrate how <span class="math inline">\(X^2 \sim \chi^2(1)\)</span>. We previously tested <span class="math inline">\(X\)</span> against Shapiro-Wilk and Anderson-Darling
test to indicate normality.</p>
<pre class="r"><code>set.seed(1)
X &lt;- rnorm(n=100, mean=0, sd=1)</code></pre>
<p>First, we raised <span class="math inline">\(X\)</span> to the power of two.</p>
<pre class="r"><code>X2 &lt;- X^2</code></pre>
<pre class="r"><code>shapiro.test(X2)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  X2
## W = 0.7, p-value = 5e-13</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.chisq.demo1-1.png" width="100%" /></p>
<p>Shapiro-Wilk and visual examination suggested <span class="math inline">\(X^2\)</span> does not follow a normal
distribution. But, does it follow a <span class="math inline">\(\chi^2\)</span> distribution?</p>
<pre class="r"><code>ks.test(X2, pchisq, df=1)</code></pre>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  X2
## D = 0.1, p-value = 0.2
## alternative hypothesis: two-sided</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.chisq.demo2-1.png" width="100%" /></p>
<p>So we can safely conclude that raising a normal distribution to the power of
two resulted in a <span class="math inline">\(\chi^2\)</span> distribution with a parameter of <span class="math inline">\(k=1\)</span>.</p>
</div>
</div>
<div id="central-limit-theorem" class="section level1">
<h1>Central Limit Theorem</h1>
<p>So far, we have learnt sampling distributions, where we see different
implementation on probability functions in discrete and continuous variables.
We are also able to compute the mean and standard deviation based on their
parameters. It just happened that the sample mean follows a normal
distribution ;) Central Limit Theorem (CLT) delineates such an occurrence,
applicable to both discrete and continuous distributions.</p>
<p><span class="math display">\[\bar{X} \xrightarrow{d} N \bigg(\mu, \frac{\sigma}{\sqrt{n}} \bigg) as\ n \to \infty\]</span></p>
<p>The equation above basically tells us that the sample mean distribution
<span class="math inline">\(\bar{X}\)</span> have a convergence upon random variables to follow a normal
distribution with mean of <span class="math inline">\(\mu\)</span> and standard deviation of <span class="math inline">\(\frac{\sigma^2}{n}\)</span>.
Each <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> corresponds to computed values acquired from the
sampled distribution.</p>
<p>The caveat (or the perk) with CLT is we need a sufficiently large sample number
of <span class="math inline">\(n\)</span>. The number of sample depends on data skewness, where more skewed data
requires a higher value of <span class="math inline">\(n\)</span>. We can determine <span class="math inline">\(n\)</span> using a simple simulation,
using following steps:</p>
<ol style="list-style-type: decimal">
<li>Choose any distribution</li>
<li>Generate <span class="math inline">\(n\)</span> random numbers using specified parameters</li>
<li>Compute the mean and variance based on previous parameters <span class="math inline">\(\to\)</span> Use it to generate a normal distribution</li>
<li>Reuse the parameters to re-iterate step 2</li>
<li>Conduct the simulation for an arbitrary number of times (e.g. for convenience, 1000)</li>
<li>Calculate mean from all generated data <span class="math inline">\(\to\)</span> Make a histogram and compare it with step 3</li>
<li>Does not fit normal distribution? <span class="math inline">\(\to\)</span> Increase <span class="math inline">\(n\)</span></li>
</ol>
<p>Why should we care about CLT if we can do statistical inferences without it? In
a research settings, we may find a differing average values despite following
the exact procedure. It could be frustrating! Knowing CLT, we can prove the
difference is indeed within expectation. As a final excerpt, CLT describes a
tendency of a mean to follow normal distribution, but it requires a sufficient
number of <span class="math inline">\(n\)</span>.</p>
</div>
