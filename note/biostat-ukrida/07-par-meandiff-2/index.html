---
author: lam
title: "Parametric: Mean in Multiple Groups"
weight: 7
description: >

  The limitation  when using T-Test is its inability to directly compare multiple
  group at once. Often times, we are interested to see whether our groups of
  interest present with at least on differing average value. To alleviate this
  issue, we can assign a generalized form of a T-Test. We will do so by analyzing
  between and within group variances. This analysis resulted in the sum of square
  with two degree of freedoms, one coming from the number of groups and another
  from the calculation of withing group variability.

summary: >

  The limitation  when using T-Test is its inability to directly compare multiple
  group at once. Often times, we are interested to see whether our groups of
  interest present with at least on differing average value. To alleviate this
  issue, we can assign a generalized form of a T-Test. We will do so by analyzing
  between and within group variances. This analysis resulted in the sum of square
  with two degree of freedoms, one coming from the number of groups and another
  from the calculation of withing group variability.

date: 2020-11-09
categories: ["statistics", "ukrida"]
tags: ["R", "hypothesis"]
slug: 07-par-meandiff-2
csl: ../harvard.csl
bibliography: ../ref.bib
draft: false
---



<p><a href="https://lamurian.rbind.io/note/biostat-ukrida/07-par-meandiff-2/slide">Slide</a></p>
<p>The limitation when using T-Test is its inability to directly compare multiple
group at once. Often times, we are interested to see whether our groups of
interest present with at least on differing average value. To alleviate this
issue, we can assign a generalized form of a T-Test. We will do so by analyzing
between and within group variances. This analysis resulted in the sum of square
with two degree of freedoms, one coming from the number of groups and another
from the calculation of withing group variability.</p>
<div id="one-way-anova" class="section level1">
<h1>One-Way ANOVA</h1>
<p>ANOVA, as it stands for the Analysis of Variance, calculates the relative
variability observed within and between the groups. The perk of using ANOVA is
its ability to distinguish mean difference among multiple groups at once. As it
measures the sum of square differences, the statistics result in ANOVA follows
an F-distribution. One-way ANOVA is a specific type which you only assign one
grouping mechanism as your independent variable. As we have previously done,
we will first declare the hypotheses to test on.</p>
<ul>
<li><span class="math inline">\(H_0:\)</span> The population mean of all groups are all equal</li>
<li><span class="math inline">\(H_a:\)</span> The population mean of all groups are not all equal</li>
</ul>
<p>Please note that on the following equation the denominator is a residual of our
observation. F-Test is a quotient of between to within differences, both
measured as a mean square, basically a sum of square divided by the degree of
freedom. In our equation, <span class="math inline">\(\bar{X}_j\)</span> is the mean of a group while <span class="math inline">\(\bar{X}\)</span> is
the mean of all sampled groups. The value of <span class="math inline">\(\nu = k-1\)</span> for between comparison
and <span class="math inline">\(\nu = N-k\)</span> for within comparison, where <span class="math inline">\(k\)</span> stands for the number of
groups and n <span class="math inline">\(N\)</span> for the total number of sample.</p>
<p><span class="math display">\[F = \frac{\displaystyle \sum_{j=1}^{k} n_j (\bar{X}_j - \bar{X})^2 / (k-1)}{\displaystyle \sum_{j=1}^k \sum_{i=1}^N (X_i - \bar{X}_j)^2 / (N-k)}\]</span></p>
<p>In performing a one-way ANOVA, we need to ascertain the following assumptions:</p>
<ul>
<li>I.I.D</li>
<li>Normally distributed</li>
<li>Homogeneity of variance</li>
</ul>
<p>As we may recall, we can use Shapiro-Wilk’s statistics as a normality test and
Levene’s as homogeneity of variance test.</p>
<div id="f-distribution" class="section level2">
<h2>F-Distribution</h2>
<p>As the T-distribution derives from the normal distribution to estimate the
mean, the F-distribution derives from the <span class="math inline">\(\chi^2\)</span> distribution to define a
ratio of two variances. We may have a vague recollection that the <span class="math inline">\(\chi^2\)</span>
distribution is a specific case of Gamma distribution from the exponential
family. Interestingly, when we have a data <span class="math inline">\(X \sim N(0, 1)\)</span>, then
<span class="math inline">\(X^2 \sim \chi^2(1)\)</span>. Similarly, when both of normality and homogeneity of
variance assumptions fulfilled, the statistics <span class="math inline">\(T^2 = F\)</span>. That is an
interesting relationship you may occur to observe when performing an one-way
ANOVA in two groups and comparing the result to the Student’s T-Test.</p>
<p><span class="math display">\[\begin{align}
P(X=x) &amp; = \frac{(\frac{r_1}{r_2})^{\frac{r_1}{2}} \Gamma[(r_1 + r_2) / 2] x^{\frac{r_1}{2}-1}}{\Gamma(\frac{r_1}{2}) \Gamma(\frac{r_2}{2}) [1 + (\frac{r_1 \cdot x}{r_2})]^{\frac{(r_1+r_2)}{2}}} \tag{PDF}\\
X &amp; \sim F(r_1, r_2) \tag{notation} \\
F &amp; = \frac{U / r_1}{V / r_2}
\end{align}\]</span></p>
<p>Previous equations describe the PDF and notation we can use in defining the
F-distribution, where <span class="math inline">\(r_i\)</span> is the degree of freedom, while <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are
<span class="math inline">\(\chi^2\)</span> distribution.</p>
</div>
<div id="example-please" class="section level2">
<h2>Example, please?</h2>
<p>Throughout the lecture, we will use example datasets from <code>R</code>. To demonstrate
the one-way ANOVA, we are using a <code>PlantGrowth</code> dataset. This dataset simply
describes dried plants weight measured under multiple conditions:</p>
<ul>
<li><code>ctrl</code> for the control (no treatment)</li>
<li><code>trt1</code> is the group receiving treatment 1</li>
<li><code>trt2</code> is the group receiving treatment 2</li>
</ul>
<p>We shall first examine our dataset:</p>
<pre class="r"><code># Data structure
str(PlantGrowth)</code></pre>
<pre><code>## &#39;data.frame&#39;:    30 obs. of  2 variables:
##  $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...
##  $ group : Factor w/ 3 levels &quot;ctrl&quot;,&quot;trt1&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<pre class="r"><code># Descriptive statistics
with(PlantGrowth, tapply(weight, group, summary)) %&gt;% {do.call(rbind, .)}</code></pre>
<pre><code>##      Min. 1st Qu. Median Mean 3rd Qu. Max.
## ctrl 4.17    4.55   5.15 5.03    5.29 6.11
## trt1 3.59    4.21   4.55 4.66    4.87 6.03
## trt2 4.92    5.27   5.44 5.53    5.73 6.31</code></pre>
<p>Then assessed our assumptions of normality and homogeneity of intergroup
variance:</p>
<pre class="r"><code>with(PlantGrowth, tapply(weight, group, shapiro.test)) %&gt;%
    lapply(broom::tidy) %&gt;% lapply(data.frame) %&gt;% {do.call(rbind, .)}</code></pre>
<pre><code>##      statistic p.value                      method
## ctrl     0.957   0.747 Shapiro-Wilk normality test
## trt1     0.930   0.452 Shapiro-Wilk normality test
## trt2     0.941   0.564 Shapiro-Wilk normality test</code></pre>
<pre class="r"><code>car::leveneTest(weight ~ group, data=PlantGrowth)</code></pre>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##       Df F value Pr(&gt;F)
## group  2    1.12   0.34
##       27</code></pre>
<p>Seeing both p-value from Shapiro-Wilk and Levene’s test being greater than
0.05, we can conclude that our data follow a normal distribution with roughly
equal variances. We may conduct ANOVA by issuing the following command:</p>
<pre class="r"><code>aov.res1 &lt;- aov(weight ~ group, data=PlantGrowth)
anova(aov.res1)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: weight
##           Df Sum Sq Mean Sq F value Pr(&gt;F)  
## group      2   3.77   1.883    4.85  0.016 *
## Residuals 27  10.49   0.389                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We obtain a significant result, brilliant! But we still need to further analyze
its residual by plotting them against the fitted value:</p>
<pre class="r"><code>par(mfrow=c(2, 2)); plot(aov.res1)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/one.way4-1.png" width="100%" /></p>
<p>The residual of our model roughly follows a normal distribution, despite the
presence of outliers in data index number 4, 15 and 17 (see in figure entitled
normal Q-Q). On the left side of our figure, we can observe a homogeneous
residual variances, indicated by the absence of megaphone nor fan effect, i.e.
the residual distributed evenly for all measured groups. Of course we need to
present our data visually as to make it easier for our reader to understand:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.one.way-1.png" width="100%" /></p>
<p>So far, we can conclude that conducting ANOVA requires the following steps to
take:</p>
<ol style="list-style-type: decimal">
<li>Normality <span class="math inline">\(\to\)</span> Shapiro-Wilk or Anderson-Darling test</li>
<li>Homogeneity of variance <span class="math inline">\(\to\)</span> Levene’s test</li>
<li>Do modelling and interpretation</li>
<li>Check model goodness of fit</li>
</ol>
<p>In the next section we will see how to apply such steps to perform two-way and
repeated measure ANOVA.</p>
</div>
</div>
<div id="two-way-anova" class="section level1">
<h1>Two-Way ANOVA</h1>
<p>After demonstrating one-way ANOVA, we are quite satisfied with how it measures
mean differences across multiple groups. However, in some cases, we may need to
consider more than one grouping mechanism. This way, we can also control for
interaction happening among groups. Two-way ANOVA provides a way to perform
such a tedious task. As a side note, we can refer a two-way ANOVA as a
factorial ANOVA. Since it is an extension to one-way ANOVA, it has similar
assumptions of:</p>
<ul>
<li>I.I.D</li>
<li>Normality</li>
<li>Homogeneity of variance</li>
</ul>
<div id="example-please-1" class="section level2">
<h2>Example, please?</h2>
<p>To establish an understanding on conducting a factorial ANOVA, we will use
<code>ToothGrowth</code> dataset as it is readily available in <code>R</code>. This dataset also
presents in JASP if you prefer to use them for a convenient purpose.
The <code>ToothGrwoth</code> dataset has three variables of</p>
<ul>
<li><code>len</code>: Tooth length</li>
<li><code>supp</code>: Supplement given</li>
<li><code>dose</code>: Supplement dose</li>
</ul>
<p>As we previously did in one-way ANOVA, we shall start our inference by
inspecting the data:</p>
<pre class="r"><code># Data structure
str(ToothGrowth)</code></pre>
<pre><code>## &#39;data.frame&#39;:    60 obs. of  3 variables:
##  $ len : num  4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ...
##  $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ dose: num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...</code></pre>
<pre class="r"><code># Set dose as a factor
ToothGrowth$dose %&lt;&gt;% factor(levels=c(0.5, 1.0, 2.0))</code></pre>
<p>Then we may want to measure the descriptive statistics:</p>
<pre class="r"><code># Grouped by supplement type
with(ToothGrowth, tapply(len, supp, summary)) %&gt;% {do.call(rbind, .)}</code></pre>
<pre><code>##    Min. 1st Qu. Median Mean 3rd Qu. Max.
## OJ  8.2    15.5   22.7 20.7    25.7 30.9
## VC  4.2    11.2   16.5 17.0    23.1 33.9</code></pre>
<pre class="r"><code># Grouped by prescribed dose
with(ToothGrowth, tapply(len, dose, summary)) %&gt;% {do.call(rbind, .)}</code></pre>
<pre><code>##     Min. 1st Qu. Median Mean 3rd Qu. Max.
## 0.5  4.2    7.22   9.85 10.6    12.2 21.5
## 1   13.6   16.25  19.25 19.7    23.4 27.3
## 2   18.5   23.53  25.95 26.1    27.8 33.9</code></pre>
<p>And of course, a normality test:</p>
<pre class="r"><code># Grouped by supplement type
with(ToothGrowth, tapply(len, supp, shapiro.test)) %&gt;%
    lapply(broom::tidy) %&gt;% lapply(data.frame) %&gt;% {do.call(rbind, .)}</code></pre>
<pre><code>##    statistic p.value                      method
## OJ     0.918  0.0236 Shapiro-Wilk normality test
## VC     0.966  0.4284 Shapiro-Wilk normality test</code></pre>
<pre class="r"><code># Grouped by prescribed dose
with(ToothGrowth, tapply(len, dose, shapiro.test)) %&gt;%
    lapply(broom::tidy) %&gt;% lapply(data.frame) %&gt;% {do.call(rbind, .)}</code></pre>
<pre><code>##     statistic p.value                      method
## 0.5     0.941   0.247 Shapiro-Wilk normality test
## 1       0.931   0.164 Shapiro-Wilk normality test
## 2       0.978   0.902 Shapiro-Wilk normality test</code></pre>
<p>Oh no! The data in <code>OJ</code> group does not follow a normal distribution, what
should we do? Fret not ;) With an increasing number of sample, we often see
that our data will not follow a normal distribution. In such a case, it is
always good to perform a visual examination to determine whether our data
contains outliers. In case of extreme outliers, we may opt to remove such an
entry so that we can safely conduct an ANOVA.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/two.way4-1.png" width="100%" /></p>
<p>We have outliers in data index 7 and 8, but it is still within a safe boundary
of normal distribution theoretical quantiles. So it is alright to continue our
inference and fulfilling the homogeneity of variance assumption.</p>
<pre class="r"><code># Grouped by supplement type
car::leveneTest(len ~ supp, data=ToothGrowth)</code></pre>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##       Df F value Pr(&gt;F)
## group  1    1.21   0.28
##       58</code></pre>
<pre class="r"><code># Grouped by prescribed dose
car::leveneTest(len ~ dose, data=ToothGrowth)</code></pre>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##       Df F value Pr(&gt;F)
## group  2    0.65   0.53
##       57</code></pre>
<p>The data presents with a homogeneous intergroup variance, so we can fit an
ANOVA model to understand the mean difference.</p>
<pre class="r"><code>aov.res2 &lt;- aov(len ~ supp + dose, data=ToothGrowth)
anova(aov.res2)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: len
##           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## supp       1    205     205    14.0 0.00043 ***
## dose       2   2426    1213    82.8 &lt; 2e-16 ***
## Residuals 56    820      15                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Next, we will do a goodness of fit evaluation by analyzing the residual.</p>
<pre class="r"><code>par(mfrow=c(2,2)); plot(aov.res2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/two.way7-1.png" width="100%" /></p>
<p>Then we can plot our statistically-proven mean difference using a nice figure.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.two.way-1.png" width="100%" /></p>
</div>
</div>
<div id="repeated-measure-anova" class="section level1">
<h1>Repeated Measure ANOVA</h1>
<p>When the independence assumption violated, we need to perform a paired instead
of unpaired test. It is the same thing in ANOVA, where we can apply both of
one-way and factorial repeated measure ANOVA. However, please bear in mind that
we still assume normality without the presence of an extreme outlier. Instead
of homogeneity of variance, we need to prove a sphericity assumption, a
homogeneity of between group variances.</p>
<div id="example-please-2" class="section level2">
<h2>Example, please?</h2>
<p>Up to this point, we are pretty much accustomed to measuring mean differences
and have a practical knowledge on how ANOVA works. For this demonstration, we
will use <code>ChickWeight</code> dataset to conduct a repeated measure ANOVA. In this
dataset, we have an observation of chicken weight in grams measured overtime.
As we will se on the initial inspection, the dataset contains following
variables:</p>
<ul>
<li><code>weight</code>: chicken weight in grams</li>
<li><code>Time</code>: number of days since birth</li>
<li><code>Chick</code>: unique identifier on the chicken</li>
<li><code>Diet</code>: type of diet given</li>
</ul>
<pre class="r"><code># Data structure
str(ChickWeight)</code></pre>
<pre><code>## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;:   578 obs. of  4 variables:
##  $ weight: num  42 51 59 64 76 93 106 125 149 171 ...
##  $ Time  : num  0 2 4 6 8 10 12 14 16 18 ...
##  $ Chick : Ord.factor w/ 50 levels &quot;18&quot;&lt;&quot;16&quot;&lt;&quot;15&quot;&lt;..: 15 15 15 15 15 15 15 15 15 15 ...
##  $ Diet  : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  - attr(*, &quot;formula&quot;)=Class &#39;formula&#39;  language weight ~ Time | Chick
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##  - attr(*, &quot;outer&quot;)=Class &#39;formula&#39;  language ~Diet
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##  - attr(*, &quot;labels&quot;)=List of 2
##   ..$ x: chr &quot;Time&quot;
##   ..$ y: chr &quot;Body weight&quot;
##  - attr(*, &quot;units&quot;)=List of 2
##   ..$ x: chr &quot;(days)&quot;
##   ..$ y: chr &quot;(gm)&quot;</code></pre>
<p>As previously conducted, we shall test for data normality.</p>
<pre class="r"><code>with(ChickWeight, tapply(weight, Time, shapiro.test)) %&gt;%
    lapply(broom::tidy) %&gt;% lapply(data.frame) %&gt;% {do.call(rbind, .)}</code></pre>
<pre><code>##    statistic  p.value                      method
## 0      0.890 2.22e-04 Shapiro-Wilk normality test
## 2      0.873 6.86e-05 Shapiro-Wilk normality test
## 4      0.973 3.15e-01 Shapiro-Wilk normality test
## 6      0.982 6.48e-01 Shapiro-Wilk normality test
## 8      0.980 5.77e-01 Shapiro-Wilk normality test
## 10     0.981 6.16e-01 Shapiro-Wilk normality test
## 12     0.983 6.86e-01 Shapiro-Wilk normality test
## 14     0.973 3.25e-01 Shapiro-Wilk normality test
## 16     0.986 8.30e-01 Shapiro-Wilk normality test
## 18     0.991 9.75e-01 Shapiro-Wilk normality test
## 20     0.991 9.68e-01 Shapiro-Wilk normality test
## 21     0.986 8.69e-01 Shapiro-Wilk normality test</code></pre>
<p>The data in <span class="math inline">\(t_0\)</span> and <span class="math inline">\(t_2\)</span> does not seem right. We have a few options to
select on:</p>
<ul>
<li>Transform the data, at the cost of losing information on measured units</li>
<li>Filter and delete row entry with existing outliers contributing to skewness</li>
<li>Drop out observed variables (columns) not fulfilling the normality assumption</li>
</ul>
<p>Since we have so many observation of interest, it will not be a problem to drop
two columns out of 12 observation periods. Of course, you may want to suit your
choice with how you would like to interpret your data. But here we shall
demonstrate a convenient method of handling non-normal variables.</p>
<pre class="r"><code>tbl &lt;- subset(ChickWeight, subset=!{ChickWeight$Time==0 | ChickWeight$Time==2})
with(tbl, tapply(weight, Time, shapiro.test)) %&gt;%
    lapply(broom::tidy) %&gt;% lapply(data.frame) %&gt;% {do.call(rbind, .)}</code></pre>
<pre><code>##    statistic p.value                      method
## 4      0.973   0.315 Shapiro-Wilk normality test
## 6      0.982   0.648 Shapiro-Wilk normality test
## 8      0.980   0.577 Shapiro-Wilk normality test
## 10     0.981   0.616 Shapiro-Wilk normality test
## 12     0.983   0.686 Shapiro-Wilk normality test
## 14     0.973   0.325 Shapiro-Wilk normality test
## 16     0.986   0.830 Shapiro-Wilk normality test
## 18     0.991   0.975 Shapiro-Wilk normality test
## 20     0.991   0.968 Shapiro-Wilk normality test
## 21     0.986   0.869 Shapiro-Wilk normality test</code></pre>
<p>Now they look better :) Next, we will check for any extreme outlier.</p>
<pre class="r"><code># Use `weight` variable as a reference to identify outliers
rstatix::identify_outliers(tbl, variable=&quot;weight&quot;)</code></pre>
<pre><code>##   weight Time Chick Diet is.outlier is.extreme
## 1    331   21    21    2       TRUE      FALSE
## 2    327   20    34    3       TRUE      FALSE
## 3    341   21    34    3       TRUE      FALSE
## 4    332   18    35    3       TRUE      FALSE
## 5    361   20    35    3       TRUE      FALSE
## 6    373   21    35    3       TRUE      FALSE
## 7    321   21    40    3       TRUE      FALSE
## 8    322   21    48    4       TRUE      FALSE</code></pre>
<p>Sure there are outliers, but none of them are extreme. With a large number of
observation, ANOVA is quite robust against data not following a normal
distribution. However, the presence of extreme outliers or severely-skewed data
compromise this robustness. We have fulfilled two of the three assumptions, now
we need to fit in our ANOVA model and assess for its sphericity assumption
using Mauchly’s test.</p>
<pre class="r"><code>rstatix::anova_test(data=tbl, dv=weight, wid=Chick, within=Time)</code></pre>
<pre><code>## ANOVA Table (type III tests)
## 
## $ANOVA
##   Effect DFn DFd   F         p p&lt;.05   ges
## 1   Time   9 396 196 8.03e-140     * 0.617
## 
## $`Mauchly&#39;s Test for Sphericity`
##   Effect        W         p p&lt;.05
## 1   Time 6.67e-13 1.48e-209     *
## 
## $`Sphericity Corrections`
##   Effect   GGe     DF[GG]    p[GG] p[GG]&lt;.05   HFe      DF[HF]    p[HF]
## 1   Time 0.136 1.22, 53.7 3.46e-21         * 0.137 1.24, 54.43 1.92e-21
##   p[HF]&lt;.05
## 1         *</code></pre>
<p>Interpreting the Mauchly’s test, we may conclude that our data violated the
sphericity assumption. As such, we will use the corrected p-value as presented
at the bottom of our results.</p>
<pre class="r"><code>rstatix::anova_test(data=tbl, dv=weight, wid=Chick, within=Time, between=Diet)</code></pre>
<pre><code>## ANOVA Table (type III tests)
## 
## $ANOVA
##      Effect DFn DFd      F         p p&lt;.05   ges
## 1      Diet   3  41   5.01  5.00e-03     * 0.185
## 2      Time   9 369 234.20 1.21e-146     * 0.685
## 3 Diet:Time  27 369   3.52  2.77e-08     * 0.089
## 
## $`Mauchly&#39;s Test for Sphericity`
##      Effect        W         p p&lt;.05
## 1      Time 8.13e-13 1.97e-190     *
## 2 Diet:Time 8.13e-13 1.97e-190     *
## 
## $`Sphericity Corrections`
##      Effect  GGe      DF[GG]    p[GG] p[GG]&lt;.05   HFe      DF[HF]    p[HF]
## 1      Time 0.14 1.26, 51.64 9.97e-23         * 0.142 1.28, 52.51 4.53e-23
## 2 Diet:Time 0.14 3.78, 51.64 1.40e-02         * 0.142 3.84, 52.51 1.40e-02
##   p[HF]&lt;.05
## 1         *
## 2         *</code></pre>
<p>What have we learnt so far? We know that to conduct a parametric hypothesis
test we need to fulfill some stringent assumptions. Our demonstration so far
has delineated the difference between a one-way and factorial ANOVA. In case of
independence assumption violation, we need to employ a repeated measure ANOVA.</p>
</div>
</div>
<div id="post-hoc-analysis" class="section level1">
<h1>Post-Hoc Analysis</h1>
<p>When assessing the mean difference across multiple groups, it might be
intriguing to further infer which pairwise comparison actually present with a
statistically significant mean difference. After conducting an ANOVA, it is
always a good practice to apply a post-hoc analysis using Tukey’s range test,
or also called a Tukey’s Honest Significant Difference (HSD). After
understanding the role of T-test and ANOVA, it is reasonably tempting to use
T-test right after ANOVA. However, it turned out we have a higher probability
of getting a type-I statistical error when applying multiple T-Test as a
post-hoc analysis.</p>
<p>Tukey’s HSD alleviates such a problem by calculating the actual mean difference
between two groups and dividing the residual by a square root of a quotient
between the mean square within and the number of samples in one of observed
group. Please note that Tukey’s HSD is best applied for cases where numbers of
observation for each group are equal. In case you have differing number of
sample size, we need to use a Tukey-Kramer method. You may also want to
consider using the Scheffe test in the case of unequal sample size. However, to
keep this post brief, we will stick with Tukey’s HSD and Tukey-Kramer.</p>
<div id="example-please-3" class="section level2">
<h2>Example, please?</h2>
<p>We will have the previous result of one-way ANOVA as our example. By imputing
the model into <code>TukeyHSD</code> function in <code>R</code>, we will have the following output.</p>
<pre class="r"><code>aov.res1 %&gt;% anova() %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 2 × 6
##   term         df sumsq meansq statistic p.value
##   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 group         2  3.77  1.88       4.85  0.0159
## 2 Residuals    27 10.5   0.389     NA    NA</code></pre>
<pre class="r"><code>TukeyHSD(aov.res1) %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 3 × 7
##   term  contrast  null.value estimate conf.low conf.high adj.p.value
##   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
## 1 group trt1-ctrl          0   -0.371   -1.06      0.320      0.391 
## 2 group trt2-ctrl          0    0.494   -0.197     1.19       0.198 
## 3 group trt2-trt1          0    0.865    0.174     1.56       0.0120</code></pre>
<p>As another demonstration, we can also use the our second model using factorial
ANOVA.</p>
<pre class="r"><code>aov.res2 %&gt;% anova() %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 3 × 6
##   term         df sumsq meansq statistic   p.value
##   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 supp          1  205.  205.       14.0  4.29e- 4
## 2 dose          2 2426. 1213.       82.8  1.87e-17
## 3 Residuals    56  820.   14.7      NA   NA</code></pre>
<pre class="r"><code>TukeyHSD(aov.res2) %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 4 × 7
##   term  contrast null.value estimate conf.low conf.high adj.p.value
##   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
## 1 supp  VC-OJ             0    -3.70    -5.68     -1.72    4.29e- 4
## 2 dose  1-0.5             0     9.13     6.22     12.0     1.32e- 9
## 3 dose  2-0.5             0    15.5     12.6      18.4     7.31e-12
## 4 dose  2-1               0     6.37     3.45      9.28    6.98e- 6</code></pre>
</div>
</div>
<div id="effect-size" class="section level1">
<h1>Effect Size</h1>
<p>When conducting a mean difference using T-test methods (note the plural), we
also computed the effect size using Cohen’s <span class="math inline">\(d\)</span>. However, <span class="math inline">\(d\)</span> is limited to
measure distance in a two-sample mean difference. As ANOVA is a generalized
form of a T-test, measuring its effect size also requires a different approach.
Here we shall discuss <span class="math inline">\(\eta^2\)</span> and partial <span class="math inline">\(\eta^2\)</span> to measure the effect size
on ANOVA and repeated ANOVA, respectively.</p>
<p><span class="math display">\[\begin{align}
\eta^2 &amp;= \frac{SS_{effect}}{SS_{total}} \\
Partial\ \eta^2 &amp;= \frac{SS_{effect}}{SS_{effect} + SS_{error}}
\end{align}\]</span></p>
<div id="example-please-4" class="section level2">
<h2>Example, please?</h2>
<pre class="r"><code>heplots::etasq(aov.res1, partial=FALSE)</code></pre>
<pre><code>##           eta^2
## group     0.264
## Residuals    NA</code></pre>
<pre class="r"><code>heplots::etasq(aov.res2, partial=FALSE)</code></pre>
<pre><code>##            eta^2
## supp      0.0595
## dose      0.7029
## Residuals     NA</code></pre>
<p>After acquiring the effect size, we can use it to calculate the power of our
statistical inference.</p>
</div>
<div id="power-analysis" class="section level2">
<h2>Power analysis</h2>
<pre class="r"><code>pwr::pwr.anova.test(k=3, n=10, f=0.264)</code></pre>
<pre><code>## 
##      Balanced one-way analysis of variance power calculation 
## 
##               k = 3
##               n = 10
##               f = 0.264
##       sig.level = 0.05
##           power = 0.213
## 
## NOTE: n is number in each group</code></pre>
<pre class="r"><code>pwr::pwr.anova.test(k=2, n=30, f=0.0595)</code></pre>
<pre><code>## 
##      Balanced one-way analysis of variance power calculation 
## 
##               k = 2
##               n = 30
##               f = 0.0595
##       sig.level = 0.05
##           power = 0.0739
## 
## NOTE: n is number in each group</code></pre>
<pre class="r"><code>pwr::pwr.anova.test(k=3, n=20, f=0.7029)</code></pre>
<pre><code>## 
##      Balanced one-way analysis of variance power calculation 
## 
##               k = 3
##               n = 20
##               f = 0.703
##       sig.level = 0.05
##           power = 0.999
## 
## NOTE: n is number in each group</code></pre>
</div>
</div>
