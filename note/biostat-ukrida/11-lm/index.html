---
author: lam
title: "Linear Model"
weight: 11
description: >

  Upon seeing a linear trend between two variables, we may guess a potential
  value of interest given a specific data point. When doing so, we are attempting
  an inference on predicted outcomes based on observed events. Given a linear
  trend between multiple variables, such an inference is the basic foundation of
  a linear model, i.e. a mathematical construct in examining the dependent
  variable using known independent variables.

summary: >

  Upon seeing a linear trend between two variables, we may guess a potential
  value of interest given a specific data point. When doing so, we are attempting
  an inference on predicted outcomes based on observed events. Given a linear
  trend between multiple variables, such an inference is the basic foundation of
  a linear model, i.e. a mathematical construct in examining the dependent
  variable using known independent variables.

date: 2020-12-07
categories: ["statistics", "ukrida"]
tags: ["R", "hypothesis"]
slug: 11-lm
csl: ../harvard.csl
bibliography: ../ref.bib
draft: false
---



<p><a href="https://lamurian.rbind.io/note/biostat-ukrida/11-lm/slide">Slide</a></p>
<p>Upon seeing a linear trend between two variables, we may guess a potential
value of interest given a specific data point. When doing so, we are attempting
an inference on predicted outcomes based on observed events. Given a linear
trend between multiple variables, such an inference is the basic foundation of
a linear model, i.e.Â a mathematical construct in examining the dependent
variable using known independent variables.</p>
<div id="fundamental-concepts" class="section level1">
<h1>Fundamental Concepts</h1>
<p>As its name suggests, a <em>linear</em> model utilizes <em>linear</em> algebra to make
prediction on <span class="math inline">\(y\)</span> given the value of <span class="math inline">\(x\)</span>. Assuming a linear trend between a
pair of <span class="math inline">\((x_i, y_i)\)</span>, then we may calculate <span class="math inline">\(y_i\)</span> as a result of an arbitrary
constant <span class="math inline">\(\beta\)</span> multiplied to <span class="math inline">\(x\)</span>. The value of <span class="math inline">\(\beta\)</span> reflects our estimate
on how <span class="math inline">\(x\)</span> influences <span class="math inline">\(y\)</span>, which termed as an estimate or a slope. However,
sometimes we may observe that <span class="math inline">\(x_i=0\)</span> results in <span class="math inline">\(y_i \neq 0\)</span>. To make a better
estimate, we need to take into account such an occurrence using an intercept
<span class="math inline">\(\beta_0\)</span>. Some limitations in doing an inference, or even designing a model in
general, we can only take into account the estimated value <span class="math inline">\(E(y_i)\)</span>, where our
prediction <span class="math inline">\(\hat{y}\)</span> poses a randomly-distributed error <span class="math inline">\(\epsilon\)</span>. The term
error does not mean an error in computation, rather, it is the residuals
between <span class="math inline">\(\hat{y}_i\)</span> and <span class="math inline">\(y\)</span>.</p>
<p><span class="math display">\[\begin{align}
\hat{y}_i &amp;= \beta_0 + \beta_1 x_i \\
y_i &amp;= \hat{y} + \epsilon_i
\end{align}\]</span></p>
<p>In short, we may learn that a linear model is an extension to the correlation
analysis, where it explains the linearity between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. The important
bit about statistical model is its capability to act as an explanatory and
predictive model. An explanatory model aims to explain how independent
variables influence the dependent variable. In doing so, we need to minimize
the multicollinearity so that we are confident each of our independent
variables directly influence the dependent variable without receiving
influences from other independent variables. On the other hand, a predictive
model aims to deliver the closest possible prediction of <span class="math inline">\(\hat{y}_i\)</span>, i.e.
minimizing the error <span class="math inline">\(\epsilon\)</span>.</p>
<div id="example-please" class="section level2">
<h2>Example, please?</h2>
<pre class="r"><code>str(iris)</code></pre>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<pre class="r"><code>subset(iris, select=-Species) %&gt;% cor()</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length         1.00       -0.12         0.87        0.82
## Sepal.Width         -0.12        1.00        -0.43       -0.37
## Petal.Length         0.87       -0.43         1.00        0.96
## Petal.Width          0.82       -0.37         0.96        1.00</code></pre>
<p>There is a seemingly good correlation between sepal length and petal width. We
will use both variables as our dependent and independent variable,
respectively. We will initially perform a correlation analysis, using the
<code>cor.test()</code> function in <code>R</code>.</p>
<pre class="r"><code>with(iris, cor.test(Sepal.Length, Petal.Length))</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  Sepal.Length and Petal.Length
## t = 22, df = 148, p-value &lt;2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.83 0.91
## sample estimates:
##  cor 
## 0.87</code></pre>
<p>Then we will model the linearity between sepal length and petal length using
<code>lm()</code>.</p>
<pre class="r"><code>mod1 &lt;- lm(Sepal.Length ~ Petal.Length, data=iris) %T&gt;% {print(summary(.))}</code></pre>
<pre><code>## 
## Call:
## lm(formula = Sepal.Length ~ Petal.Length, data = iris)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.2468 -0.2966 -0.0152  0.2768  1.0027 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    4.3066     0.0784    54.9   &lt;2e-16 ***
## Petal.Length   0.4089     0.0189    21.6   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.41 on 148 degrees of freedom
## Multiple R-squared:  0.76,   Adjusted R-squared:  0.758 
## F-statistic:  469 on 1 and 148 DF,  p-value: &lt;2e-16</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plt.mod1-1.png" width="90%" /></p>
<p>We have previously discussed that the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_i\)</span>
determine how to predict <span class="math inline">\(y\)</span> using <span class="math inline">\(x\)</span>. But, how do we calculate each value of
<span class="math inline">\(\beta\)</span>? We can derive <span class="math inline">\(\beta\)</span> from our data using two methods: Ordinary
Least Square (OLS) and Maximum Likelihood Estimation (MLE). Both techniques are
solvable using a partial derivative or matrices operation. In this note, we
will only consider the former to build a firm foundation on how they work.</p>
<p>When calculating the optimal value of <span class="math inline">\(\beta\)</span>, we aim to find the best line
fitting our data. Optimal <span class="math inline">\(\beta\)</span> will result in the least amount of error
<span class="math inline">\(\epsilon\)</span>. It also generalizes the model to adapt to unforeseen data. Since a
statistical model calculates the expected value of <span class="math inline">\(E(y)\)</span>, we can anticipate
that a model will draw a line passing through the centroid of
<span class="math inline">\((\bar{x},\bar{y})\)</span>.</p>
</div>
<div id="ordinary-least-square" class="section level2">
<h2>Ordinary Least Square</h2>
<p>The Ordinary Least Square (OLS) method looks for the most appropriate model by
minimizing the error <span class="math inline">\(\epsilon\)</span>. A pair of <span class="math inline">\((x_i, y_i)\)</span> are constants relative
to the index <span class="math inline">\(i\)</span>. In case of a simple linear regression, the error only depends
on <span class="math inline">\(\beta_{0,1}\)</span>. We can use partial derivatives to find the optimal values of
<span class="math inline">\(\beta\)</span> in minimizing the error <span class="math inline">\(\epsilon\)</span>.</p>
<p><span class="math display">\[\begin{align}
\epsilon &amp;= \displaystyle \sum_{i=1}^n (y_i - \hat{y}_i)2 \\
&amp;= \displaystyle \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2
\end{align}\]</span></p>
<p>You may recall learning about derivatives in you high school years. The partial
derivative is similar, the only difference is it allows a partial assignment.
When partially solving one variable, we regard the other as a constant.</p>
<p><span class="math display">\[\begin{align}
\epsilon &amp;= \displaystyle \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2 \\
\frac{\partial \epsilon}{\partial \beta_0} &amp;= \displaystyle \sum_{i=1}^n -2 (y_i - (\beta_0 + \beta_1 x_i)) \\
\frac{\partial \epsilon}{\partial \beta_1} &amp;= \displaystyle \sum_{i=1}^n -2 x_i (y_i - (\beta_0 + \beta_1 x_i))
\end{align}\]</span></p>
<p>We shall solve the <span class="math inline">\(\beta_0\)</span> as follow:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \epsilon}{\partial \beta_0} = \displaystyle \sum_{i=1}^n -2 (y_i - (\beta_0 + \beta_1 x_i)) &amp;= 0\\
-2 \bigg( \displaystyle \sum_{i=1}^n y_i - \sum_{i=1}^n \beta_0 - \sum_{i=1}^n \beta_1 x_i \bigg) &amp;= 0 \\
\displaystyle \sum_{i=1}^n y_i - n \beta_0 - \sum_{i=1}^n \beta_1 x_i &amp;= 0 \\
\beta_0 &amp;= \frac{1}{n} \bigg( \displaystyle \sum_{i=1}^n y_i - \beta_1 \sum_{i=1}^n x_i \bigg) \\
\beta_0 &amp;= \bar{y} - \beta_1 \bar{x}
\end{align}\]</span></p>
<p>Then we can proceed by solving the <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \epsilon}{\partial \beta_1} = \displaystyle \sum_{i=1}^n -2 x_i (y_i - (\beta_0 + \beta_1 x_i)) &amp;= 0 \\
-2 \bigg( \displaystyle \sum_{i=1}^n x_i (y_i - (\bar{y} - \beta_1 \bar{x} + \beta_1 x_i)) \bigg) &amp;= 0\\
\displaystyle \sum_{i=1}^n x_i (y_i - \bar{y}) - \sum_{i=1}^n \beta_1 x_i (x_i - \bar{x}) &amp;= 0 \\
\beta_1 \displaystyle \sum_{i=1}^n x_i (x_i - \bar{x}) &amp;= \sum_{i=1}^n x_i (y_i - \bar{y}) \\
\beta_1 &amp;= \displaystyle \sum_{i=1}^n \frac{x_i(y_i - \bar{y})}{x_i(x_i - \bar{x})} \\
\beta_1 &amp;= \displaystyle \sum_{i=1}^n \frac{(x_i - \bar{x})(y_i - \bar{y})}{(x_i - \bar{x})(x_i - \bar{x})} \\
\end{align}\]</span></p>
<p>As you may have noticed, <span class="math inline">\(\beta_1\)</span> solved into the quotient of covariance and
variance.</p>
</div>
</div>
